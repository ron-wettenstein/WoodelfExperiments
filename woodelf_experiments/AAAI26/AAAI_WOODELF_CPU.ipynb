{"cells":[{"cell_type":"markdown","metadata":{"id":"jJTxPcri_zok"},"source":["#Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10105,"status":"ok","timestamp":1752919995748,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"rmwF4lp3_sPo","outputId":"9cf8f0fb-62aa-4c81-975e-fc4a2e0ce791"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"]}],"source":["!pip install shap\n","!pip install xgboost"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"8-A1K_wP_3f4","executionInfo":{"status":"ok","timestamp":1752984167977,"user_tz":-180,"elapsed":5582,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}}},"outputs":[],"source":["import xgboost as xgb\n","import shap\n","import pandas as pd\n","import numpy as np\n","from typing import Union, Dict, Optional, Tuple, Set, List\n","from math import factorial\n","import time\n","from copy import copy\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.metrics import accuracy_score, f1_score\n","import scipy\n","\n","# For GPU execution\n","import cupy as cp"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16024,"status":"ok","timestamp":1752984184016,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"3syONkVz_3dU","outputId":"709fe090-18b4-46c7-8844-069bb961f6e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Useful if you run this on google colab and downloaded the data into your drive.\n","# If you run the notebook in other environment remove these lines and change the 'pd.read_csv()' function in this notebook to read from\n","# where you saved you data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"HToRzUbmATa6"},"source":["## Environment Note\n","All the code below, that works with the EEEI-CIS fraud data will run perfectly in the free version of Google Colab.\n","The free version machine have 12GB RAM and this is enough for this part of the notebook.\n","\n","Next, we will load the KDD-Cup dataset. As it includes millions of rows we need more than 12GB to run the algorithm.\n","This part of the notebook needs the 50GB CPU machine. It is available for subscribed members for Google Colab (toggle the High-RAM option in Runtime->Change runtime type button).\n","\n","If you only bought computation units, but not a subscription, you can still run the full notebook using the v2-8-TPU machine. This machine have more then enough RAM and it is pretty cheap. Its CPU is a bit slower though, so you will get a slower runtimes.\n","\n","Final note: preformance in Colab depends on the machine allocated (the CPU option can allocate several types of machines) and on load balancing inside Colab (when more users use the system running times might be slower). So the running times of our algorithms and the shap python package can very. The difference between our approach and the current state-of-the-art is big enough to be noticed, but the excat running times can be slightly different from the ones stated our paper."]},{"cell_type":"markdown","metadata":{"id":"xeNs6a-AApPC"},"source":["# WOODELF Code"]},{"cell_type":"markdown","metadata":{"id":"8_OfQXy_ApGl"},"source":["### Decision Tree Ensemble Representation and Loading\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcpAxQwlASyo"},"outputs":[],"source":["from shap.explainers._tree import XGBTreeModelLoader # Use the shap package's XGBoost loading. this is cheating, I know...\n","\n","class DecisionTreeNode:\n","    \"\"\"\n","    Represent a decision tree node. Recursively the root node builds the tree structure (the root node knows it children and so on).\n","    Include several useful tree functions like BFS and n.split(c).\n","    \"\"\"\n","\n","    def __init__(\n","            self, feature_name: str, value: float, right: Optional[\"DecisionTreeNode\"], left: Optional[\"DecisionTreeNode\"], nan_go_left=True, index: int=None, cover=None,\n","            feature_contribution_replacement_values=None,\n","        ):\n","        \"\"\"\n","        See decision tree definition in the paper (Def. 6)\n","        The tree split function is a bit different as XGBoost also support NaN values.\n","        The split function is \"go left if df[feature_name]<value or (nan_go_left and df[feature_name] == NaN)\".\n","        Right and left parameters can be a DesicionTreeNode if this node is an inner node or None if the node is a leaf. (The leaf weight will be saved as the 'value')\n","        Cover is an optional parameter, it includes how many rows in the train set reached this node.\n","        \"\"\"\n","        self.index=index\n","        self.feature_name = feature_name\n","        self.value = float(value)\n","        self.right = right\n","        self.left = left\n","        self.nan_go_left = nan_go_left\n","        self.cover = cover\n","        self.consumer_pattern_to_characteristic_wdnf = None\n","        self.pc_pb_to_cube = None\n","        self.feature_contribution_replacement_values = feature_contribution_replacement_values\n","        self.parent = -1\n","        self.depth=None\n","\n","    def shall_go_left(self, row):\n","        \"\"\"\n","        This is the n.split(c) defined in Def.6\n","        \"\"\"\n","        if self.nan_go_left:\n","            return (row[self.feature_name] < self.value) | row[self.feature_name].isna()\n","        else:\n","            return row[self.feature_name] < self.value\n","\n","    def shall_go_right(self, row):\n","        return ~self.shall_go_left(row)\n","\n","    def GPU_shall_go_left(self, row):\n","        \"\"\"\n","        This is the n.split(c) defined in Def.6\n","        \"\"\"\n","        if self.nan_go_left:\n","            return (row[self.feature_name] < self.value) | cp.isnan(row[self.feature_name])\n","        else:\n","            return row[self.feature_name] < self.value\n","\n","    def GPU_shall_go_right(self, row):\n","        return ~self.GPU_shall_go_left(row)\n","\n","    def is_leaf(self):\n","        return self.right is None and self.left is None\n","\n","    def is_almost_leaf(self):\n","        return not self.is_leaf() and (self.right.is_leaf() or self.left.is_leaf())\n","\n","    def predict(self, data):\n","        if self.is_leaf():\n","            return pd.Series(self.value, index=data.index)\n","        return self.shall_go_left(data) * self.left.predict(data) + self.shall_go_right(data) * self.right.predict(data)\n","\n","    def bfs(self, including_myself: bool = True, including_leaves: bool = True):\n","        \"\"\"\n","        Return all the node children (and the node itself) in BFS order. The indexes should be in an increasing order.\n","        \"\"\"\n","        if self.is_leaf():\n","            return [self] if including_myself and including_leaves else []\n","\n","        children = [self] if including_myself else []\n","        nodes_to_visit = []\n","        if self.right is not None:\n","            nodes_to_visit.append(self.right)\n","        if self.left is not None:\n","            nodes_to_visit.append(self.left)\n","\n","        while len(nodes_to_visit) > 0:\n","            current_node = nodes_to_visit.pop(0)\n","            if current_node.right is not None:\n","                nodes_to_visit.append(current_node.right)\n","            if current_node.left is not None:\n","                nodes_to_visit.append(current_node.left)\n","\n","            if current_node.is_leaf():\n","                if including_leaves:\n","                    children.append(current_node)\n","            else:\n","                children.append(current_node)\n","\n","        return children\n","\n","    def get_all_leaves(self):\n","        children = self.bfs(including_leaves=True)\n","        return [node for node in children if node.is_leaf()]\n","\n","    def get_all_almost_leaves(self):\n","        children = self.bfs(including_leaves=True)\n","        return [node for node in children if node.is_almost_leaf()]\n","\n","    def get_all_features(self):\n","        inner_nodes = self.bfs(including_leaves=False)\n","        return set(n.feature_name for n in inner_nodes)\n","\n","    def get_all_leaves_with_path_to_root(self):\n","        nodes_to_visit = [(self, [])]\n","        leaves = []\n","        while len(nodes_to_visit) > 0:\n","            current_node, current_path_to_root = nodes_to_visit.pop(0)\n","            for next_node in [current_node.right, current_node.left]:\n","                next_node_obj = (next_node, current_path_to_root + [current_node.feature_name])\n","                if next_node.is_leaf():\n","                    leaves.append(next_node_obj)\n","                else:\n","                    nodes_to_visit.append(next_node_obj)\n","        return leaves\n","\n","    def __repr__(self):\n","        if self.is_leaf():\n","            return f\"{self.index} (cover: {self.cover}): leaf with value {self.value}\"\n","        return f\"{self.index} (cover: {self.cover}): {self.feature_name} < {self.value}\"\n","\n","def load_xgboost_tree(tree, features):\n","    \"\"\"\n","    Given an XGBoost Regressor tree, parse it and build a DecisionTreeNode object with it structure.\n","    Use the Tree object returned by the shap package's XGBTreeModelLoader class (given as the 'tree' parameter).\n","    The function also gets the training features.\n","    \"\"\"\n","    nodes = {}\n","    for index in range(len(tree.thresholds)):\n","        threshold = tree.thresholds[index]\n","        leaf_value = tree.values[index][0]\n","        if threshold == 0 and leaf_value != 0:\n","            value = leaf_value\n","        else:\n","            value = threshold\n","        nan_go_left = (tree.children_left[index] == tree.children_default[index])\n","        cover = tree.node_sample_weight[index]\n","        feature_index = tree.features[index]\n","        nodes[index] = DecisionTreeNode(\n","            feature_name=features[feature_index], value=value, right=None, left=None,\n","            nan_go_left=nan_go_left, index=index, cover=cover\n","        )\n","\n","    for index in range(len(tree.thresholds)):\n","        child_left = tree.children_left[index]\n","        child_right = tree.children_right[index]\n","\n","        if child_left != -1:\n","            nodes[index].left = nodes[child_left]\n","            nodes[child_left].parent = nodes[index]\n","        if child_right != -1:\n","            nodes[index].right = nodes[child_right]\n","            nodes[child_right].parent = nodes[index]\n","\n","    nodes[0].depth = tree.max_depth\n","    return nodes[0]\n","\n","\n","def load_xgboost_model(model, features):\n","    \"\"\"\n","    Load an XGBoost regressor tree (utilizing the shap python package parsing object)\n","    \"\"\"\n","    loader = XGBTreeModelLoader(model)\n","    return [load_xgboost_tree(t, features) for t in loader.get_trees()]"]},{"cell_type":"markdown","metadata":{"id":"2uLDFJehAzqi"},"source":["### Shapley/Banzhaf values and interaction values computation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8op_Nu3Axcx"},"outputs":[],"source":["def nCk(n, k):\n","    return factorial(n) // (factorial(k) * factorial(n-k))\n","\n","class CubeCharacteristicFunctionMetric(object):\n","    \"\"\"\n","    An abstruct class that calculate a metric on a cube/clause characteristic function.\n","    You can implement this class (override the calc_metric function) and then use this class and the WOODELF algorithm\n","    to calculate your metric effiecently on large background datasets.\n","\n","    Here, the metrics that inherit this class are: Shapley values, Shapley interaction values, Banzhaf values and Banzhaf interaction values\n","    \"\"\"\n","    INTERACTION_VALUE = False\n","    INTERACTION_VALUE_ONE_SIDE = False\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        raise NotImplemented()\n","\n","class ShapleyValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the linear-time formula for Shapley value computation on WDNF/WCNF, see Formula 3 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = False\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","\n","        s = s_plus | s_minus\n","        shapley_values = {}\n","\n","        # The new simple shapley values formula\n","        if len(s_plus) > 0:\n","            contribution = (1 / (len(s_plus) * nCk(len(s), len(s_plus))))\n","            for must_exist_feature in s_plus:\n","                shapley_values[must_exist_feature] = contribution\n","\n","        if len(s_minus) > 0:\n","            contribution = -1 / (len(s_minus) * nCk(len(s), len(s_minus)))\n","            for must_be_missing_feature in s_minus:\n","                shapley_values[must_be_missing_feature] = contribution\n","\n","        return shapley_values\n","\n","class ShapleyInteractionValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the formulas for Shapley interaction values computation on WDNF/WCNF, see Table 1 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = True\n","    INTERACTION_VALUE_ONE_SIDE = True\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","\n","        shapley_values = {}\n","        s = s_plus | s_minus\n","        if len(s_plus) > 0:\n","            # i,j in S+\n","            if len(s_plus) > 1:\n","                # 0.5 because the shapley interaction values in the shap package are actually divided by 2....\n","                contribution = 0.5 / ((len(s_plus) - 1) * nCk(len(s) - 1, len(s_plus) - 1))\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_plus:\n","                        if must_exists_feature < other_feature:\n","                            shapley_values[(must_exists_feature, other_feature)] = contribution\n","\n","            # i in S+   j in S-\n","            if len(s_minus) > 0:\n","                contribution = -0.5 / (len(s_minus) * nCk(len(s) - 1, len(s_minus)))\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_minus:\n","                        if must_exists_feature < other_feature:\n","                            shapley_values[(must_exists_feature, other_feature)] = contribution\n","\n","        if len(s_minus) > 0:\n","            # i,j in S-\n","            if len(s_minus) > 1:\n","                contribution = 0.5 / ((len(s_minus) - 1) * nCk(len(s) - 1, len(s_minus) - 1))\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_minus:\n","                        if must_be_missing_feature < other_feature:\n","                            shapley_values[(must_be_missing_feature, other_feature)] = contribution\n","            # i in S-   j in S+\n","            if len(s_plus) > 0:\n","                contribution = -0.5 / (len(s_plus) * nCk(len(s) - 1, len(s_plus)))\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_plus:\n","                        if must_be_missing_feature < other_feature:\n","                            shapley_values[(must_be_missing_feature, other_feature)] = contribution\n","        return shapley_values\n","\n","\n","class BanzahfValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the linear-time formula for Banzhaf value computation on WDNF/WCNF, see Formula 6 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = False\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","\n","        s = s_plus | s_minus\n","        banzhaf_values = {}\n","\n","        s_plus_contribution = 1 / (2 ** (len(s) - 1))\n","        s_minus_contribution = -s_plus_contribution\n","        # The new simple shapley values formula\n","        if len(s_plus) > 0:\n","            for must_exist_feature in s_plus:\n","                banzhaf_values[must_exist_feature] = s_plus_contribution\n","\n","        if len(s_minus) > 0:\n","            for must_be_missing_feature in s_minus:\n","                banzhaf_values[must_be_missing_feature] = s_minus_contribution\n","\n","        return banzhaf_values\n","\n","\n","class BanzhafInteractionValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the formulas for Banzhaf interaction values computation on WDNF/WCNF, see Formula 7 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = True\n","    INTERACTION_VALUE_ONE_SIDE = True\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","        banzhaf_values = {}\n","\n","        contribution = (1 / (2 ** (len(s_plus) + len(s_minus) - 2)))\n","\n","        s = s_plus | s_minus\n","        if len(s_plus) > 0:\n","            # i,j in S+\n","            if len(s_plus) > 1:\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_plus:\n","                        if must_exists_feature < other_feature:\n","                            banzhaf_values[(must_exists_feature, other_feature)] = contribution\n","\n","            # i in S+   j in S-\n","            if len(s_minus) > 0:\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_minus:\n","                        if must_exists_feature < other_feature:\n","                            banzhaf_values[(must_exists_feature, other_feature)] = -contribution\n","\n","        if len(s_minus) > 0:\n","            # i,j in S-\n","            if len(s_minus) > 1:\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_minus:\n","                        if must_be_missing_feature < other_feature:\n","                            banzhaf_values[(must_be_missing_feature, other_feature)] = contribution\n","            # i in S-   j in S+\n","            if len(s_plus) > 0:\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_plus:\n","                        if must_be_missing_feature < other_feature:\n","                            banzhaf_values[(must_be_missing_feature, other_feature)] = -contribution\n","        return banzhaf_values"]},{"cell_type":"markdown","metadata":{"id":"LkktslZPA5R1"},"source":["### WOODELF CODE!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2eegCGdBv0x"},"outputs":[],"source":["class PathToValuesMatrix():\n","    \"\"\"\n","    An object that in charge of creating the M matrix for every leaf and feature.\n","    It takes the features along the root-to-leaf path and build the matrix (lines 7-16 in WOODELF pseudo code)\n","    The class also utilize the fact that the matrix only depends on the repitting sequence of the features along the path.\n","    For example the feature repetition sequence of [\"weight\", \"pluse\", \"age\", \"sex\", \"pluse\", \"sex\"] is [1, 2, 3, 4, 2, 4].\n","    All feature lists with this feature repetition sequence have the same set of matrixes.\n","\n","    This cache mechanism is improvement 2 in Sec. 9.1\n","    \"\"\"\n","    def __init__(self, metric: CubeCharacteristicFunctionMetric):\n","        self.metric = metric\n","\n","        self.cached_used = 0\n","        self.cache_miss = 0\n","        self.cache = {}\n","\n","\n","    def get_values_matrixes(self, features_in_path: List[str]):\n","        \"\"\"\n","        Apply the CubeCharacteristicFunctionMetric object (the v function), to create the matrixes M.\n","        Use the cache when possible, and update the cache with the created matrixes\n","        \"\"\"\n","        frs = self.get_feature_repetition_sequence(features_in_path)\n","\n","        frs_tuple = tuple(frs)\n","        if frs_tuple in self.cache:\n","            self.cached_used += 1\n","            matrixes = self.cache[frs_tuple]\n","        else:\n","            self.cache_miss += 1\n","            pc_pb_to_cube = self.map_patterns_to_cube(frs)\n","            matrixes = self.build_patterns_to_values_matrix(pc_pb_to_cube, self.metric, len(features_in_path))\n","            self.cache[frs_tuple] = matrixes\n","\n","        if not self.metric.INTERACTION_VALUE:\n","            matrixes_for_the_given_features = {features_in_path[index]: matrixes[index] for index in matrixes}\n","        else:\n","            matrixes_for_the_given_features = {}\n","            for feature_index_1, feature_index_2 in matrixes:\n","                the_right_index = (features_in_path[feature_index_1], features_in_path[feature_index_2])\n","\n","                # The feature_appearance can change the order of a feature pair (f1,f2) in one sided interaction metric, here we fix this.\n","                if self.metric.INTERACTION_VALUE_ONE_SIDE and features_in_path[feature_index_1] > features_in_path[feature_index_2]:\n","                    the_right_index = (features_in_path[feature_index_2], features_in_path[feature_index_1])\n","\n","                matrixes_for_the_given_features[the_right_index] = matrixes[(feature_index_1, feature_index_2)]\n","\n","        return matrixes_for_the_given_features\n","\n","    @staticmethod\n","    def get_feature_repetition_sequence(features_in_path: List[str]):\n","        \"\"\"\n","        Generate the feature repetition sequence.\n","        The math is simple, the feature at index i is replaced by i\n","        unless it appeared before in the sequance, in that case it will be represented by the index it already received.\n","\n","        Examples:\n","        [\"sex\", \"pluse\", \"age\", \"weight\", \"heart_rate\", \"sugar_in_blood\"] => [1, 2, 3, 4, 5, 6]\n","        [\"weight\", \"pluse\", \"age\", \"sex\", \"pluse\", \"sex\"] => [1, 2, 3, 4, 2, 4]\n","        \"\"\"\n","        feature_to_index = {}\n","        frs = []\n","        for i, feature in enumerate(features_in_path):\n","            if feature in feature_to_index:\n","                frs.append(feature_to_index[feature])\n","            else:\n","                feature_to_index[feature] = i\n","                frs.append(i)\n","\n","        return frs\n","\n","\n","    @staticmethod\n","    def build_patterns_to_values_matrix(dl, metric: CubeCharacteristicFunctionMetric, path_length):\n","        \"\"\"\n","        Apply the CubeCharacteristicFunctionMetric object (the v function), to create the matrixes M.\n","        include lines 12-16 in WOODELF pseudo code.\n","        dl is the returned mapping from the map_patterns_to_cube function\n","        \"\"\"\n","        matrix_details = {}\n","        for pc in dl:\n","            for pb in dl[pc]:\n","                s_plus, s_minus = dl[pc][pb]\n","                values = metric.calc_metric(s_plus, s_minus)\n","                for feature in values:\n","                    # Implement the line \"M[l][feature][p_c][p_b] = value\" in an efficient way that utilize the sparsity of M.\n","                    if feature not in matrix_details:\n","                        matrix_details[feature] = {\"pcs\": [], \"pbs\": [], \"values\": []}\n","                    matrix_details[feature][\"pcs\"].append(pc)\n","                    matrix_details[feature][\"pbs\"].append(pb)\n","                    matrix_details[feature][\"values\"].append(values[feature])\n","\n","        matrixs = {}\n","        for feature in matrix_details:\n","            # Save M as a sparse matrix (Improvement 1 in Sec. 9.1)\n","            matrix_values = (matrix_details[feature][\"values\"], (matrix_details[feature][\"pcs\"], matrix_details[feature][\"pbs\"]))\n","            matrixs[feature] = scipy.sparse.coo_matrix(matrix_values, shape=(2**path_length, 2**path_length), dtype=np.float32).tocsc()\n","        return matrixs\n","\n","    @staticmethod\n","    def map_patterns_to_cube(features_in_path: List[str]):\n","        \"\"\"\n","        The function MapPatternsToCube from Sect. 5 of the article.\n","        :params tree: The decision tree\n","        :params current_wdnf_table: The format is: wdnf_table[consumer_decision_pattern][background_decision_pattern] = (cube_positive_literals, cube_negative_literals)\n","        \"\"\"\n","        updated_wdnf_table = {0: {0: (set(), set())}}\n","        current_wdnf_table = None\n","        for feature in features_in_path:\n","            current_wdnf_table = updated_wdnf_table\n","            updated_wdnf_table = {}\n","            for consumer_pattern in current_wdnf_table:\n","                updated_wdnf_table[consumer_pattern * 2 + 0] = {}\n","                updated_wdnf_table[consumer_pattern * 2 + 1] = {}\n","                for background_pattern in current_wdnf_table[consumer_pattern]:\n","                    # Get the current cube (the possitive and negated literals) of the consumer and background patterns\n","                    s_plus, s_minus = current_wdnf_table[consumer_pattern][background_pattern]\n","                    # Implement the 4 rules\n","                    updated_wdnf_table[consumer_pattern * 2 + 1][background_pattern * 2 + 0] = (s_plus | {feature}, s_minus) # Rule 1\n","                    updated_wdnf_table[consumer_pattern * 2 + 0][background_pattern * 2 + 1] = (s_plus, s_minus | {feature}) # Rule 2\n","                    updated_wdnf_table[consumer_pattern * 2 + 1][background_pattern * 2 + 1] = (s_plus, s_minus) # Rule 3\n","\n","        return updated_wdnf_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IW3Nim0A488"},"outputs":[],"source":["\n","\n","def get_int_dtype_from_depth(depth):\n","    \"\"\"\n","    The decision pattern, when encoded as a number, have a bit for each node of the root-to-leaf-path.\n","    Choose the dtype according to the tree depth (a.k.a the max pattern length).\n","\n","    This is improvement 5 of Sec. 9.1\n","    \"\"\"\n","    if depth <= 8:\n","        return np.uint8\n","    if depth <= 16:\n","        return np.uint16\n","    if depth <= 32:\n","       return np.uint32\n","    return np.uint64\n","\n","\n","def GPU_get_int_dtype_from_depth(depth):\n","    \"\"\"\n","    Like get_int_dtype_from_depth but return CuPy types.\n","    \"\"\"\n","    if depth <= 8:\n","        return cp.uint8\n","    if depth <= 16:\n","        return cp.uint16\n","    if depth <= 32:\n","       return cp.uint32\n","    return cp.uint64\n","\n","\n","def calc_decision_patterns(tree, data, depth, GPU=False):\n","    \"\"\"\n","    An effiecent implementation of the CalcDecisionPatterns from Sec. 4 of the paper.\n","    \"\"\"\n","    # Use a tight uint type for efficiency. This is improvement 5 of Sec. 9.1\n","    int_dtype = GPU_get_int_dtype_from_depth(depth) if GPU else get_int_dtype_from_depth(depth)\n","\n","    leaves_patterns_dict = {} # This is the P_leaves mentioned in the paper\n","    inner_nodes_patterns_dict = {} # This is P_all\n","    if GPU:\n","        data_length = len(data[list(data.keys())[0]])\n","        inner_nodes_patterns_dict[tree.index] = cp.zeros(data_length, dtype=int_dtype)\n","    else:\n","        inner_nodes_patterns_dict[tree.index] = pd.Series(0, index=data.index).to_numpy().astype(int_dtype)\n","\n","    for current_node in tree.bfs():\n","        if current_node.is_leaf():\n","            leaves_patterns_dict[current_node.index] = inner_nodes_patterns_dict[current_node.index]\n","            continue\n","\n","        if GPU:\n","            left_bool_condition = current_node.GPU_shall_go_left(data)\n","            left_condition = left_bool_condition # .to_numpy().astype(int_dtype)\n","            right_condition = ~left_bool_condition # (~left_bool_condition).to_numpy().astype(int_dtype)\n","        else:\n","            left_bool_condition = current_node.shall_go_left(data)\n","            left_condition = left_bool_condition.to_numpy().astype(int_dtype)\n","            right_condition = (~left_bool_condition).to_numpy().astype(int_dtype)\n","        my_pattern = inner_nodes_patterns_dict[current_node.index]\n","        shifted_my_pattern = (my_pattern << 1)\n","        inner_nodes_patterns_dict[current_node.left.index] = shifted_my_pattern + left_condition\n","        inner_nodes_patterns_dict[current_node.right.index] = shifted_my_pattern + right_condition\n","    return leaves_patterns_dict\n","\n","\n","def preprocess_tree_background(tree: DecisionTreeNode, background_data: pd.DataFrame, depth: int, path_to_matrixes_calculator: PathToValuesMatrix, GPU=False):\n","    \"\"\"\n","    Run all the preprocessing needed given a tree and a background_data.\n","    Include lines 2-21 of the pseudo-code.\n","    \"\"\"\n","    background_patterns_matrix = calc_decision_patterns(tree, background_data, depth, GPU)\n","\n","    # Build f, implements lines 3-4 of the pseudo-code\n","    Frq_b = {}\n","    visited_leaves_parents = {}\n","    data_length = len(background_data) if not GPU else len(background_data[list(background_data.keys())[0]])\n","    for leaf, features_in_path in tree.get_all_leaves_with_path_to_root():\n","        if leaf.parent.index not in visited_leaves_parents:\n","            # np.bincount is a faster way to implement value_counts that uses the fact all decision patterns are integers between 0 and 2**depth\n","            if GPU:\n","                Frq_b[leaf.index] = cp.bincount(background_patterns_matrix[leaf.index], minlength=2**len(features_in_path))\n","                Frq_b[leaf.index] = Frq_b[leaf.index] / data_length\n","                Frq_b[leaf.index] = cp.asnumpy(Frq_b[leaf.index])\n","            else:\n","                Frq_b[leaf.index] = np.bincount(background_patterns_matrix[leaf.index], minlength=2**len(features_in_path))\n","                Frq_b[leaf.index] = Frq_b[leaf.index] / data_length\n","            visited_leaves_parents[leaf.parent.index] = Frq_b[leaf.index]\n","        else:\n","            # neighboor leaves have similar patterns (only the last bit is different)\n","            # For efficiency we reuse the frequencies computed for the neighboor.\n","\n","            # Given leaves l_i, l_{i+1} s.t. there is an inner node n where n.left = l_i and n.right=l_{i+1}.\n","            # The decision pattern of any consumer c in leaf l_i is the same as in leaf l_{i+1} except for the last bit which is different.\n","            # For example if the pattern of c and l_i is 010011011101 then the pattern of c and l_{i+1} is 010011011100 (the 1 in the end is replaced with 0)\n","            # Let the frequencies of l_i be [f1,f2,f3,f4,....,f_{n-1}, f_n], we can these conclude that the frequencies of l_{i+1} are [f2,f1,f4,f3,....,f_n, f_{n-1}].\n","            # We can find them by swapping any pair of numbers in the array.\n","            # The code below utilize this fact for efficiency - this saved half of the bincount opperations.\n","            # This trick is part of improvement 3 in Sec. 9.1 (this is the improvement to line 4)\n","            neighboor_frq = visited_leaves_parents[leaf.parent.index]\n","            frqs = []\n","            for i in range(0, len(neighboor_frq), 2):\n","                frqs.append(neighboor_frq[i+1])\n","                frqs.append(neighboor_frq[i])\n","            Frq_b[leaf.index] = np.array(frqs, dtype=np.float32)\n","\n","    for leaf, features_in_path in tree.get_all_leaves_with_path_to_root():\n","        # Build M, implements lines 7-16 of the pseudo-code\n","        matrixes = path_to_matrixes_calculator.get_values_matrixes(features_in_path)\n","\n","        # Build s, implements lines 17-21 of the pseudo-code\n","        features_to_values = {}\n","        fl = Frq_b[leaf.index]\n","        if GPU and 2**len(features_in_path) < len(fl): # this trim is needed only on GPU\n","            fl = fl[:2**len(features_in_path)]\n","        for feature in matrixes:\n","            # The matrix multiplication part is implemented in CPU, the matrix is too small for the GPU overhead to be worth it.\n","            # The sparse matrix multiplication here instade of the naive dense matrix multiplication is improvement 1 in Sec. 9.1\n","            features_to_values[feature] = matrixes[feature].dot(fl) * leaf.value\n","        leaf.feature_contribution_replacement_values = features_to_values\n","    return tree\n","\n","\n","def get_cupy_data(trees: List[DecisionTreeNode], df: pd.DataFrame):\n","    \"\"\"\n","    Cast the dataframe to cupy dict mapping between columns of CuPy arrays.\n","    We only do this for feature partisipating in the trees.\n","    \"\"\"\n","    data = {}\n","    for tree in trees:\n","        for feature in tree.get_all_features():\n","            if feature not in data:\n","                data[feature] = cp.asarray(df[feature].to_numpy())\n","    return data\n","\n","\n","def calculation_given_preprocessed_tree(tree: DecisionTreeNode, data: pd.DataFrame, shapley_values = None, depth: int = 6, GPU=False):\n","    \"\"\"\n","    Use the preprocessing to efficiently calculate the desired metric (Shapley/Banzahf values or interaction values)\n","    Implements lines 22-27 of the pseudo-code\n","    \"\"\"\n","    # line 22 of the pseudo-code\n","    decision_patterns = calc_decision_patterns(tree, data, depth, GPU)\n","\n","    # lines 23-27 of the pseudo-code\n","    if shapley_values is None:\n","        shapley_values = {}\n","\n","    for almost_leaf in tree.get_all_almost_leaves():\n","        if not almost_leaf.right.is_leaf() or not almost_leaf.left.is_leaf():\n","            # If only the right or the left node is a leaf use s as is\n","            leaf = almost_leaf.right if almost_leaf.right.is_leaf() else almost_leaf.left\n","            current_edp_indexes = decision_patterns[leaf.index]\n","            replacements_arrays = leaf.feature_contribution_replacement_values\n","        else:\n","            # If both the right and left nodes are leaves use improvement 3 of Sec. 9.1 (improvement of line 26)\n","            # See also the comment in preprocess_tree_background.\n","            # Given leaves l_i, l_{i+1} s.t. there is an inner node n where n.left=l_i and n.right=l_{i+1}.\n","            # mark the s vector of feature f and leaf l_i as s_i = [a1,a2,a3,...,an]\n","            # mark the s vector of feature f and leaf l_{i+1} as s_{i+1} = [b1,b2,b3,...,bn]\n","            # A trivial numpy indexing for feature f and the two leaves is [a1,a2,a3,...,an][ patterns ] + [b1,b2,b3,...,bn][ patterns ]\n","            # Utilizing the property explained in comment in preprocess_tree_background, we can run the equivalent numpy indexing:\n","            # [a1+b2, a2+b1, a3+b4, a4+b3,...,a_{n-1}+bn, an+b_{n-1}][ patterns ]\n","            # This saves half of the numpy indexing opperations\n","            current_edp_indexes = decision_patterns[almost_leaf.left.index]\n","            replacements_arrays = almost_leaf.left.feature_contribution_replacement_values\n","            for feature, replacement_values in almost_leaf.right.feature_contribution_replacement_values.items():\n","                values = []\n","                for i in range(0, len(replacement_values), 2):\n","                    values.append(replacement_values[i+1])\n","                    values.append(replacement_values[i])\n","\n","                if feature not in replacements_arrays:\n","                    replacements_arrays[feature] = np.array(values, dtype=np.float32)\n","                else:\n","                    replacements_arrays[feature] = np.array(values, dtype=np.float32) + replacements_arrays[feature]\n","\n","        for feature, replacement_values in replacements_arrays.items():\n","            if GPU:\n","                replacements_array = cp.asarray(replacement_values)\n","            else:\n","                replacements_array = np.ascontiguousarray(replacement_values)\n","\n","            # This is where the numpy indexing occur (improvement 6 of Sec. 9.1):\n","            current_shap_contribution = replacements_array[current_edp_indexes]\n","\n","            if feature not in shapley_values:\n","                shapley_values[feature] = current_shap_contribution\n","            else:\n","                shapley_values[feature] += current_shap_contribution\n","\n","    return shapley_values\n","\n","def shapley_value_calculation_given_preprocessed_tree_ensemble(\n","        preprocess_trees: List[DecisionTreeNode], consumer_data: pd.DataFrame, global_importance: bool = False, iv_one_sized: bool = False, GPU=False):\n","    \"\"\"\n","    Run desired metric calculation on a decision tree ensemble.\n","\n","    @param global_importance: Interation values can quickly fill up all the machine RAM, as there are quadratic number of them.\n","    To be able to run the algorithm on large datasets, when global_importance=True, we save only their sum of mean absolute values across the trees.\n","    While it makes the result not useful it let us run WOODELF on large datasets and test its running time.\n","    \"\"\"\n","    shapley_values = {}\n","    for tree in tqdm(preprocess_trees, desc=\"Computing SHAP\"):\n","        if global_importance:\n","            current_shapley_values = {}\n","            calculation_given_preprocessed_tree(tree, consumer_data, shapley_values=current_shapley_values, GPU=GPU)\n","            for key in current_shapley_values:\n","                if key not in shapley_values:\n","                    shapley_values[key] = 0\n","                shapley_values[key] += np.abs(current_shapley_values[key]).sum() / len(current_shapley_values[key])\n","        else:\n","            calculation_given_preprocessed_tree(tree, consumer_data, shapley_values=shapley_values, GPU=GPU)\n","\n","    # Improvement 4 of Sec. 9.1\n","    if iv_one_sized:\n","        all_keys = list(shapley_values.keys())\n","        for f1, f2 in all_keys:\n","            assert (f2,f1) not in shapley_values\n","            shapley_values[(f2, f1)] = shapley_values[(f1, f2)]\n","\n","    return shapley_values\n","\n","def calculate_background_shap(model: xgb.Booster, consumer_data: pd.DataFrame, background_data: pd.DataFrame, metric: CubeCharacteristicFunctionMetric, global_importance: bool = False, GPU=False):\n","    \"\"\"\n","    The WOODELF algorithm!!!\n","\n","    Gets an XGBoost regressor, consumer data of size n, background data for size m and a desired metric to calculate.\n","    Compute the desired metric in O(n+m)\n","    \"\"\"\n","    model_objs = load_xgboost_model(model, list(consumer_data.columns))\n","    path_to_matrixes_calculator = PathToValuesMatrix(metric=metric)\n","    if GPU:\n","        consumer_data = get_cupy_data(model_objs, consumer_data)\n","        background_data = get_cupy_data(model_objs, background_data)\n","    preprocessed_trees = []\n","    for tree in tqdm(model_objs, desc=\"Preprocessing the trees\"):\n","        preprocessed_trees.append(preprocess_tree_background(tree, background_data, depth=tree.depth, path_to_matrixes_calculator=path_to_matrixes_calculator, GPU=GPU))\n","\n","    print(f\"cache misses: {path_to_matrixes_calculator.cache_miss}, cache used: {path_to_matrixes_calculator.cached_used}\")\n","    shapley = shapley_value_calculation_given_preprocessed_tree_ensemble(\n","        preprocessed_trees, consumer_data, global_importance, iv_one_sized = metric.INTERACTION_VALUE_ONE_SIDE, GPU=GPU\n","    )\n","    return shapley"]},{"cell_type":"markdown","metadata":{"id":"a2HIfDe5A41Z"},"source":["### Path Dependent WOODELF Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVk5ky9mA4a3"},"outputs":[],"source":["def path_dependend_frequencies(tree: DecisionTreeNode, depth):\n","    \"\"\"\n","    Estimate the frequencies of the training data using the tree cover property.\n","    Implement Formula 9 of the article for all the leaves in the provided tree.\n","    \"\"\"\n","    if tree.is_leaf():\n","        return {tree.index: []}\n","\n","    leaves_freq_dict = {}\n","    inner_nodes_freq_dict = {}\n","    inner_nodes_freq_dict[tree.index] = [1]\n","    for current_node in tree.bfs():\n","        current_node_freq = inner_nodes_freq_dict[current_node.index]\n","        if current_node.is_leaf():\n","            leaves_freq_dict[current_node.index] = np.array(\n","                inner_nodes_freq_dict[current_node.index], dtype=np.float32\n","            )\n","            continue\n","\n","        freqs_l = []\n","        for freq in current_node_freq:\n","            freqs_l.append((current_node.right.cover/current_node.cover) * freq)\n","            freqs_l.append((current_node.left.cover/current_node.cover) * freq)\n","        inner_nodes_freq_dict[current_node.left.index] = freqs_l\n","\n","        freqs_r = []\n","        for freq in current_node_freq:\n","            # Changed the order of the 2 lines here, now left is first.\n","            freqs_r.append((current_node.left.cover/current_node.cover) * freq)\n","            freqs_r.append((current_node.right.cover/current_node.cover) * freq)\n","        inner_nodes_freq_dict[current_node.right.index] = freqs_r\n","    return leaves_freq_dict\n","\n","def fast_preprocess_path_dependent_shap(tree: DecisionTreeNode, path_to_matrixes_calculator: PathToValuesMatrix, depth=6):\n","    \"\"\"\n","    Implement the preprocssing needed for Path-Dependent WOODELF\n","    \"\"\"\n","    freq = path_dependend_frequencies(tree, depth)\n","    for leaf, features_in_path in tree.get_all_leaves_with_path_to_root():\n","        matrixes = path_to_matrixes_calculator.get_values_matrixes(features_in_path)\n","        features_to_values = {}\n","        for feature in matrixes:\n","            features_to_values[feature] = matrixes[feature].dot(freq[leaf.index]) * leaf.value\n","        leaf.feature_contribution_replacement_values = features_to_values\n","    return tree\n","\n","\n","def calculate_path_dependent_shap(model, consumer_data, metric: CubeCharacteristicFunctionMetric, global_importance: bool = False, GPU=False):\n","    \"\"\"\n","    Path-Dependent WOODELF algorithm!!\n","\n","    Given a model, a consumer data and a desired metric compute the metric under the Path-Dependent assumptions.\n","    \"\"\"\n","    model_objs = load_xgboost_model(model, list(consumer_data.columns))\n","    path_to_matrixes_calculator = PathToValuesMatrix(metric=metric)\n","    if GPU:\n","        consumer_data = get_cupy_data(model_objs, consumer_data)\n","\n","    preprocessed_trees = []\n","    for tree in tqdm(model_objs, desc=\"Preprocessing the trees\"):\n","        preprocessed_trees.append(fast_preprocess_path_dependent_shap(tree, path_to_matrixes_calculator=path_to_matrixes_calculator))\n","\n","    print(f\"cache misses: {path_to_matrixes_calculator.cache_miss}, cache used: {path_to_matrixes_calculator.cached_used}\")\n","    return shapley_value_calculation_given_preprocessed_tree_ensemble(preprocessed_trees, consumer_data, global_importance, iv_one_sized=metric.INTERACTION_VALUE_ONE_SIDE, GPU=GPU)\n"]},{"cell_type":"markdown","metadata":{"id":"FFSpWTOCA4OL"},"source":["# Fraud Data Preprocessing and Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fc10V-xbQ_x_"},"outputs":[],"source":["# 1. Download train_transaction data from: https://www.kaggle.com/c/ieee-fraud-detection/data?select=train_transaction.csv\n","# 2. Save it in your google drive\n","# 3. Change the path in the 'pd.read_csv()' function to where you saved your data\n","transactions = pd.read_csv('drive/MyDrive/ShapResearch/DataAndNotebooks/Data/train_transaction.csv')\n","\n","# feature engineering - do one hot encoding for categorical features\n","transactions['is_visa'] = transactions['card4'] == 'visa'\n","transactions['is_american_express'] = transactions['card4'] == 'american express'\n","transactions['is_discover'] = transactions['card4'] == 'discover'\n","transactions['is_mastercard'] = transactions['card4'] == 'mastercard'\n","\n","transactions['is_debit'] = transactions['card6'] == 'debit'\n","\n","transactions['ProductCD_W'] = transactions['ProductCD'] == 'W'\n","transactions['ProductCD_C'] = transactions['ProductCD'] == 'C'\n","transactions['ProductCD_R'] = transactions['ProductCD'] == 'R'\n","transactions['ProductCD_H'] = transactions['ProductCD'] == 'H'\n","transactions['ProductCD_S'] = transactions['ProductCD'] == 'S'\n","\n","for i in [2,3,5,6,7,8,9]:\n","  transactions[f'M{i}'] = (transactions[f'M{i}'] == 'T').astype('int8')\n","\n","transactions['M4'] = 0\n","transactions.loc[transactions['M4'] == 'M1', 'M4'] = 1\n","transactions.loc[transactions['M4'] == 'M2', 'M4'] = 2\n","\n","transactions['gmail_hotmail_or_yahoo_email'] = transactions['P_emaildomain'].isin(['gmail.com', 'hotmail.com', 'yahoo.com'])\n","transactions['nan_email'] = transactions['P_emaildomain'].isna()\n","\n","# Add the enginered features and build the list of features to train on\n","add = ['is_visa', 'is_american_express', 'is_discover', 'is_mastercard',\n","       'is_debit',\n","       'ProductCD_W', 'ProductCD_C', 'ProductCD_R', 'ProductCD_H', 'ProductCD_S',\n","       'gmail_hotmail_or_yahoo_email', 'nan_email']\n","\n","for c in add:\n","  transactions[c] = transactions[c].astype('int8')\n","\n","remove = ['card4', 'card6', 'ProductCD', 'M1', 'P_emaildomain', 'R_emaildomain', 'TransactionID', 'isFraud', 'TransactionDT']\n","train_features = [c for c in transactions.columns if c not in remove]\n","\n","\n","# Split transactions into train and test using the TransactionDT column, the 20% highest values should be the test\n","# We could use the 'test_transaction.csv' file of the Kaggle competition for our testset but it does not have the target column ('isFraud')\n","# so we would have no way to validate our model prefromance.\n","threshold = transactions['TransactionDT'].quantile(0.8)\n","transactions_train = transactions[transactions['TransactionDT'] <= threshold]\n","transactions_test = transactions[transactions['TransactionDT'] > threshold]\n","\n","# Save RAM\n","del transactions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22179,"status":"ok","timestamp":1752920069284,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"cxUnZIN_RCPs","outputId":"4d1a392a-5cd8-43c3-992c-3291bacf3e04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9692992853998036, F1 score: 0.3914065122524337\n"]}],"source":["# Train an XGBoost model\n","\n","XGB_PARAMS = {\n","    \"objective\": \"reg:squarederror\",  # Regression task with mean squared error loss\n","    \"eval_metric\": \"rmse\",  # Evaluation metric is root mean squared error\n","    \"max_depth\": 6,  # Maximum depth of each tree\n","    \"learning_rate\": 0.1,  # Learning rate (step size shrinkage)\n","    \"subsample\": 1,  # Subsample ratio of the training instances\n","    \"colsample_bytree\": 0.8,  # Subsample ratio of columns when constructing each tree\n","    \"seed\": 123,\n","    \"nthread\": 1, # The python shap package use parallel in path dependent if the nthread is bigger then 1. Use nthread=1 to compare the approaches when both don't utilize parallelism.\n","}\n","\n","\n","def xgboost_model(X_train, y_train, params, num_rounds=100):\n","    train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n","    return xgb.train(params, train_dmatrix, num_rounds)\n","\n","# train XGBoost regressor\n","model = xgboost_model(transactions_train[train_features], transactions_train['isFraud'], XGB_PARAMS, num_rounds=100)\n","\n","transactions_train_Y = transactions_train['isFraud']\n","# Quickly evaluate our model - just so we know it was train correctly and produce meaningful predicitons\n","test_dmatrix = xgb.DMatrix(transactions_test[train_features])\n","y_pred = model.predict(test_dmatrix)\n","y_pred = pd.Series(y_pred, index=transactions_test.index)\n","print(f\"Accuracy: {accuracy_score(transactions_test['isFraud'], y_pred.round())}, F1 score: {f1_score(transactions_test['isFraud'], y_pred.round())}\")"]},{"cell_type":"markdown","metadata":{"id":"moKL2Z3dRKCN"},"source":["## WOODELF Running Times"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":301,"status":"ok","timestamp":1752920069587,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"WsPmb0eMRKWF","outputId":"65064653-2658-419f-dbad-17282b6e0969"},"outputs":[{"output_type":"stream","name":"stdout","text":["EEEI-CIS train size: 472432  EEEI-CIS test size: 118108\n"]}],"source":["fraud_trainset = transactions_train[train_features]\n","fraud_testset = transactions_test[train_features]\n","\n","del transactions_train, transactions_test\n","print(f\"EEEI-CIS train size: {len(fraud_trainset)}  EEEI-CIS test size: {len(fraud_testset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yg-3rsw8UoHq"},"outputs":[],"source":["def woodelf_running_times(model, consumer_data: pd.DataFrame, background_data: pd.DataFrame, GPU=False):\n","    results = {}\n","    for kind in ['Background', 'Path Dependent']:\n","        for metric_name, metric in {\"SHAP\": ShapleyValues(), \"SHAP IV\": ShapleyInteractionValues(), \"Banzhaf\": BanzahfValues()}.items():\n","            name = f\"{kind} {metric_name} on {'GPU' if GPU else 'CPU'}\"\n","            print(f\"\\n\\n ------ {name} ------ \")\n","            start_time = time.time()\n","            if kind == 'Background':\n","                calculate_background_shap(\n","                    model, consumer_data, background_data, metric=metric, global_importance=metric.INTERACTION_VALUE, GPU=GPU\n","                )\n","            else:\n","                calculate_path_dependent_shap(model, consumer_data, metric=metric, global_importance=metric.INTERACTION_VALUE, GPU=GPU)\n","\n","            running_time = round(time.time() - start_time, 2)\n","            print()\n","            print(f\"{name} Took: {running_time}\")\n","            results[name] = running_time\n","    return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65813,"status":"ok","timestamp":1752920135423,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"qBGgzlT-9Jv9","outputId":"d9cc6932-bfd8-44c0-afac-e621ad50e88d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," ------ Background SHAP on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:07<00:00, 14.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:05<00:00, 19.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Background SHAP on CPU Took: 12.31\n","\n","\n"," ------ Background SHAP IV on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:07<00:00, 13.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:11<00:00,  8.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Background SHAP IV on CPU Took: 18.55\n","\n","\n"," ------ Background Banzhaf on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:06<00:00, 14.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:05<00:00, 18.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Background Banzhaf on CPU Took: 12.36\n","\n","\n"," ------ Path Dependent SHAP on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 259.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:05<00:00, 18.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Path Dependent SHAP on CPU Took: 5.92\n","\n","\n"," ------ Path Dependent SHAP IV on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 143.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:10<00:00,  9.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Path Dependent SHAP IV on CPU Took: 11.28\n","\n","\n"," ------ Path Dependent Banzhaf on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 253.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:04<00:00, 20.23it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Path Dependent Banzhaf on CPU Took: 5.38\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Background SHAP on CPU': 12.31,\n"," 'Background SHAP IV on CPU': 18.55,\n"," 'Background Banzhaf on CPU': 12.36,\n"," 'Path Dependent SHAP on CPU': 5.92,\n"," 'Path Dependent SHAP IV on CPU': 11.28,\n"," 'Path Dependent Banzhaf on CPU': 5.38}"]},"metadata":{},"execution_count":13}],"source":["woodelf_running_times(model, fraud_testset, fraud_trainset, GPU=False)"]},{"cell_type":"markdown","metadata":{"id":"xPYfFZtWc60r"},"source":["## shap package running time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325050,"status":"ok","timestamp":1752920460475,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"_6NKVheqbcE_","outputId":"1872ca0d-0108-4926-f1a2-4384579c027a"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 96%|=================== | 112799/118108 [00:18<00:00]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=10: 19.74\n"]},{"output_type":"stream","name":"stderr","text":[" 99%|===================| 116819/118108 [00:39<00:00]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=20: 39.54\n"]},{"output_type":"stream","name":"stderr","text":["100%|===================| 117629/118108 [01:28<00:00]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=50: 88.92\n"]},{"output_type":"stream","name":"stderr","text":["100%|===================| 118014/118108 [02:56<00:00]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=100: 176.85\n"]}],"source":["background_shap_timings = {}\n","for head_size in [10,20,50,100]:\n","    start_time = time.time()\n","    explainer = shap.TreeExplainer(model, fraud_trainset.head(head_size), feature_perturbation='interventional')\n","    simple_shap_values = explainer.shap_values(fraud_testset)\n","    running_time = time.time() - start_time\n","    background_shap_timings[head_size] = running_time\n","    print(f\"Background SHAP m={head_size}: {round(running_time, 2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qK0z84cUc8vq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752920611667,"user_tz":-180,"elapsed":151189,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"aeda5fbc-9041-4f60-b86e-42919b918dcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path Dependent SHAP: 151.15789699554443\n"]}],"source":["start_time = time.time()\n","explainer = shap.TreeExplainer(model)\n","simple_shap_values = explainer.shap_values(fraud_testset)\n","print(\"Path Dependent SHAP: \" + str(time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4Ae-ELRdaHN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752920793361,"user_tz":-180,"elapsed":181695,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"0a1360f3-5931-4b90-9a5a-3ee2da7f5510"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path Dependent SHAP IV n=10: 10.21\n","Path Dependent SHAP IV n=20: 20.44\n","Path Dependent SHAP IV n=50: 50.26\n","Path Dependent SHAP IV n=100: 100.77\n"]}],"source":["pd_shap_iv_timings = {}\n","for head_size in [10,20,50,100]:\n","    start_time = time.time()\n","    explainer = shap.TreeExplainer(model)\n","    simple_shap_values = explainer.shap_interaction_values(fraud_testset.head(head_size))\n","    running_time = time.time() - start_time\n","    pd_shap_iv_timings[head_size] = running_time\n","    print(f\"Path Dependent SHAP IV n={head_size}: {round(running_time, 2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkTqGx6OdeAL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752920793375,"user_tz":-180,"elapsed":12,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"8f921980-cebb-4241-c2d0-a3f243e24b2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Background SHAP estimated time: 835499.3807837295\n","Path-Dependent SHAP IV estimated time: 119014.34392760276\n"]}],"source":["print(f\"Background SHAP estimated time: {background_shap_timings[100] * (len(fraud_trainset) / 100)}\")\n","print(f\"Path-Dependent SHAP IV estimated time: {pd_shap_iv_timings[100] * (len(fraud_testset) / 100)}\")"]},{"cell_type":"markdown","metadata":{"id":"ZQj9snKedvmM"},"source":["## Empirical Correctness Verification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qedKcdldxu6"},"outputs":[],"source":["def print_errors(woodelf_shapley_values, shap_package_shapley_values, tolarence=0.00001):\n","    # Verify that the SHAP values created by WOODELF and the ones created by the shap package are the same\n","    total_errors = 0\n","    for f in woodelf_shapley_values:\n","        errors = ((shap_package_shapley_values[f] - woodelf_shapley_values[f]).abs() > tolarence).sum()\n","        total_errors += errors\n","        if errors > 0:\n","            print(f, f\"{(errors / 10)}%\")\n","\n","    print(f\"\\n\\nTotal Errors {total_errors} which is {(total_errors / (len(woodelf_shapley_values) * len(woodelf_shapley_values[list(woodelf_shapley_values.keys())[0]]))) * 100}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7t5ZlfMdtEw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752920796339,"user_tz":-180,"elapsed":2957,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"5a600a30-5ddd-49ea-a71f-bd5b5529ed35"},"outputs":[{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 112.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:00<00:00, 125.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Total Errors 0 which is 0.0%\n"]}],"source":["explainer = shap.TreeExplainer(model, fraud_trainset.head(80), feature_perturbation='interventional')\n","simple_shap_values = explainer.shap_values(fraud_testset.head(1000))\n","simple_shap_importance = pd.DataFrame(simple_shap_values, index=fraud_testset.head(1000).index, columns=train_features)\n","\n","shapley_values = calculate_background_shap(\n","    model, fraud_testset.head(1000), fraud_trainset.head(80), metric=ShapleyValues()\n",")\n","\n","print_errors(shapley_values, simple_shap_importance, tolarence=0.00001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGCkXlPGhUG2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752920799159,"user_tz":-180,"elapsed":2813,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"90f16626-29ee-4fc0-e7d5-d3140e3425e9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 244.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:00<00:00, 139.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Total Errors 0 which is 0.0%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["explainer = shap.TreeExplainer(model)\n","pd_simple_shap_values = explainer.shap_values(fraud_testset.head(1000))\n","pd_simple_shap_importance = pd.DataFrame(pd_simple_shap_values, index=fraud_testset.head(1000).index, columns=train_features)\n","\n","woodelf_pd_shapley_values = calculate_path_dependent_shap(model, fraud_testset.head(1000), metric=ShapleyValues())\n","\n","print_errors(woodelf_pd_shapley_values, pd_simple_shap_importance, tolarence=0.00001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtiuE4P8hY8j"},"outputs":[],"source":["def print_errors_interaction_values(woodelf_shap_iv, shap_package_shap_iv, train_features, tolarence=0.00001):\n","    # Verify that the Shapley interaction values created by WOODELF and the ones created by the shap package are the same\n","    total_errors = 0\n","    for i, feature1 in enumerate(train_features):\n","        for j, feature2 in enumerate(train_features):\n","            if i == j:\n","                continue\n","            if (feature1, feature2) not in woodelf_shap_iv:\n","                errors = (np.abs((shap_package_shap_iv[:,i,j] - 0)) > tolarence).sum()\n","            else:\n","                errors = (np.abs((shap_package_shap_iv[:,i,j] - woodelf_shap_iv[(feature1, feature2)])) > tolarence).sum()\n","            total_errors += errors\n","\n","    print(f\"\\n\\nTotal Errors {total_errors} which is {(total_errors / (len(woodelf_shap_iv) * len(woodelf_shap_iv[list(woodelf_shap_iv.keys())[0]]))) * 100}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jo1Gnqw0hmIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752921799278,"user_tz":-180,"elapsed":1000082,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"2e099d9c-77ed-4946-ada7-cd609bd99076"},"outputs":[{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 147.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 54, cache used: 5109\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [00:00<00:00, 110.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","Total Errors 0 which is 0.0%\n"]}],"source":["explainer = shap.TreeExplainer(model)\n","simple_shap_interaction_values = explainer.shap_interaction_values(fraud_testset.head(1000))\n","\n","woodelf_pd_shapley_interaction_values = calculate_path_dependent_shap(model, fraud_testset.head(1000), metric=ShapleyInteractionValues())\n","\n","print_errors_interaction_values(woodelf_pd_shapley_interaction_values, simple_shap_interaction_values, train_features, tolarence=0.00001)"]},{"cell_type":"markdown","metadata":{"id":"wmCPG57kwm3V"},"source":["# KDD-Cup 1999: Intrusion Detection Dataset"]},{"cell_type":"markdown","metadata":{"id":"ZtJwauu1wuKD"},"source":["## Environment Note\n","The KDD-Cup is a large dataset with millions of rows. It require more the 12GB RAM to preocess.\n","\n","**From here onwards the free 12GB Colab machine will not be enough.**\n","**Use the High-RAM CPU machine.**\n","\n","Other machines with large RAM such as v2-8 TPU machine will also work (but the running times will be slower as the CPU is less advanced).\n","\n","The High-RAM CPU machine is available for subscribed members for Google Colab (toggle the High-RAM option in Runtime->Change runtime type button).\n","If you only bought computation units, but not a subscription, you can still run this using the v2-8-TPU machine."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eb7BE8OchngO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752984227479,"user_tz":-180,"elapsed":17061,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"c2a1a149-d917-48df-920e-a05aeb4c332d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4898431, 42)"]},"metadata":{},"execution_count":3}],"source":["# Step 1: Download from https://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\n","# Step 2: Ungzip the file using 7-zip (in windows) or 'gunzip kddcup.data.gz' (in linux)\n","# Step 3: Save this file in you google drive and load it from here\n","\n","columns = [\n","    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\",\n","    \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n","    \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n","    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n","    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n","    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n","    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n","    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n","    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n","    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"\n","]\n","\n","# Change the path here to where you save it in your RAM\n","detection_data = pd.read_csv('drive/MyDrive/ShapResearch/DataAndNotebooks/Data/KDD_CUP_1999/kddcup.data.corrected', names=columns)\n","detection_data.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3Zb9S5wxh0dO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752984249455,"user_tz":-180,"elapsed":21974,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"8c2379cf-17fc-4d5c-8661-8ecbbab4018b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overall 70 catagories\n"]},{"output_type":"stream","name":"stderr","text":["70it [00:17,  4.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overall 3 catagories\n"]},{"output_type":"stream","name":"stderr","text":["3it [00:00,  3.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overall 11 catagories\n"]},{"output_type":"stream","name":"stderr","text":["11it [00:02,  3.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Normal: 972781 other: 3925650\n","Train features: 121 Overall features: 127\n"]}],"source":["# Run the one hot encodeing and choose the features to train on\n","def create_one_hot_encoding(data, feature_name):\n","    vc = data[feature_name].value_counts()\n","    print(f\"Overall {len(vc)} catagories\")\n","    for catagory, count in tqdm(vc.items()):\n","        data[feature_name + \"_\" + catagory] = (data[feature_name] == catagory).astype(int)\n","\n","create_one_hot_encoding(detection_data, \"service\")\n","create_one_hot_encoding(detection_data, \"protocol_type\")\n","create_one_hot_encoding(detection_data, \"flag\")\n","\n","detection_data['target'] = (detection_data[\"label\"] != \"normal.\").astype(int)\n","print(f\"Normal: {(detection_data['target'] == 0).sum()} other: {(detection_data['target'] == 1).sum()}\")\n","\n","detc_features_to_drop = ['target', 'label', 'flag', 'service', 'protocol_type', 'service_other']\n","detection_train_features = [c for c in detection_data.columns if c not in detc_features_to_drop]\n","print(f\"Train features: {len(detection_train_features)} Overall features: {len(detection_data.columns)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5qxqsdzh0g6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752921837805,"user_tz":-180,"elapsed":3355,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"a29965ee-6992-4b65-d2cb-030024341e86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overall 65 catagories\n"]},{"output_type":"stream","name":"stderr","text":["65it [00:01, 59.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overall 3 catagories\n"]},{"output_type":"stream","name":"stderr","text":["3it [00:00, 56.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overall 11 catagories\n"]},{"output_type":"stream","name":"stderr","text":["11it [00:00, 56.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Normal: 60593 other: 250436\n"]}],"source":["# Step 1: Download the gz file from http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\n","# Step 2: Ungzip the file using 7-zip (in windows) or 'gunzip kddcup.data.gz' (in linux)\n","# Step 3: Save this file in you google drive and load it from here\n","\n","small_test_data = pd.read_csv('drive/MyDrive/ShapResearch/DataAndNotebooks/Data/KDD_CUP_1999/corrected', names=columns)\n","\n","create_one_hot_encoding(small_test_data, \"service\")\n","create_one_hot_encoding(small_test_data, \"protocol_type\")\n","create_one_hot_encoding(small_test_data, \"flag\")\n","\n","small_test_data['target'] = (small_test_data[\"label\"] != \"normal.\").astype(int)\n","print(f\"Normal: {(small_test_data['target'] == 0).sum()} other: {(small_test_data['target'] == 1).sum()}\")\n","for c in detection_train_features:\n","    if c not in small_test_data:\n","        small_test_data[c] = 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mztmKGaShnjg"},"outputs":[],"source":["detection_model = xgboost_model(detection_data[detection_train_features], detection_data['target'], XGB_PARAMS, num_rounds=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-nXo6A-xO0i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752921913084,"user_tz":-180,"elapsed":942,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"5cf9fa65-26d1-4cc6-c848-74334ee6f2fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9267721016368249, F1 score: 0.9524161704794735\n"]}],"source":["# We mainly wants to make sure the model trained on the data properly.\n","test_dmatrix = xgb.DMatrix(small_test_data[detection_train_features])\n","y_pred = detection_model.predict(test_dmatrix)\n","y_pred = pd.Series(y_pred, index=small_test_data.index)\n","print(f\"Accuracy: {accuracy_score(small_test_data['target'], y_pred.round())}, F1 score: {f1_score(small_test_data['target'], y_pred.round())}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"EOIS-znFxO4u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752984271087,"user_tz":-180,"elapsed":21630,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"dcc0e621-a8b7-45f2-ddaa-1fa67f6891f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overall 70 catagories\n"]},{"output_type":"stream","name":"stderr","text":["70it [00:10,  6.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overall 3 catagories\n"]},{"output_type":"stream","name":"stderr","text":["3it [00:00,  6.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overall 11 catagories\n"]},{"output_type":"stream","name":"stderr","text":["11it [00:01,  5.91it/s]\n"]}],"source":["# Step 1: Download the gz file from http://kdd.ics.uci.edu/databases/kddcup99/kddcup.testdata.unlabeled.gz\n","# Step 2: Ungzip the file using 7-zip (in windows) or 'gunzip kddcup.data.gz' (in linux)\n","# Step 3: Save this file in you google drive and load it from here\n","\n","columns.remove(\"label\")\n","unlabeled_data = pd.read_csv('drive/MyDrive/ShapResearch/DataAndNotebooks/Data/KDD_CUP_1999/kddcup.testdata.unlabeled', names=columns)\n","\n","create_one_hot_encoding(unlabeled_data, \"service\")\n","create_one_hot_encoding(unlabeled_data, \"protocol_type\")\n","create_one_hot_encoding(unlabeled_data, \"flag\")\n","\n","for c in detection_train_features:\n","    if c not in unlabeled_data:\n","        unlabeled_data[c] = 0"]},{"cell_type":"markdown","metadata":{"id":"CO8VzwBrxlL3"},"source":["## WOODELF Running Times"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"FBnQstccxPAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752984309762,"user_tz":-180,"elapsed":2397,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"f1d5c9fd-ebbd-4811-aa5e-afa77600c556"},"outputs":[{"output_type":"stream","name":"stdout","text":["KDD-Cup train size: 4898431  KDD-Cup test size: 2984154\n"]}],"source":["detection_trainset = detection_data[detection_train_features]\n","detection_testset = unlabeled_data[detection_train_features]\n","detection_trainset_Y = detection_data['target']\n","\n","del detection_data, unlabeled_data\n","print(f\"KDD-Cup train size: {len(detection_trainset)}  KDD-Cup test size: {len(detection_testset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qguCdrHkxrt7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752922910187,"user_tz":-180,"elapsed":974360,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"fab6c4d7-c9ec-4e4c-81d6-69892c31cd12"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," ------ Background SHAP on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:54<00:00,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 98, cache used: 4375\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [01:47<00:00,  1.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Background SHAP on CPU Took: 162.26\n","\n","\n"," ------ Background SHAP IV on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:54<00:00,  1.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 98, cache used: 4375\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [03:27<00:00,  2.08s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Background SHAP IV on CPU Took: 262.18\n","\n","\n"," ------ Background Banzhaf on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:54<00:00,  1.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 98, cache used: 4375\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [01:49<00:00,  1.09s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Background Banzhaf on CPU Took: 163.58\n","\n","\n"," ------ Path Dependent SHAP on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 250.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 98, cache used: 4375\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [01:35<00:00,  1.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Path Dependent SHAP on CPU Took: 96.47\n","\n","\n"," ------ Path Dependent SHAP IV on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 148.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 98, cache used: 4375\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [03:12<00:00,  1.92s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Path Dependent SHAP IV on CPU Took: 192.8\n","\n","\n"," ------ Path Dependent Banzhaf on CPU ------ \n"]},{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|| 100/100 [00:00<00:00, 124.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 98, cache used: 4375\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|| 100/100 [01:36<00:00,  1.04it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Path Dependent Banzhaf on CPU Took: 97.04\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Background SHAP on CPU': 162.26,\n"," 'Background SHAP IV on CPU': 262.18,\n"," 'Background Banzhaf on CPU': 163.58,\n"," 'Path Dependent SHAP on CPU': 96.47,\n"," 'Path Dependent SHAP IV on CPU': 192.8,\n"," 'Path Dependent Banzhaf on CPU': 97.04}"]},"metadata":{},"execution_count":30}],"source":["woodelf_running_times(detection_model, detection_testset, detection_trainset, GPU=False)"]},{"cell_type":"markdown","metadata":{"id":"xY-epYaY2eCw"},"source":["## shap package running time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3_ZYvFZ2hqQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752932584326,"user_tz":-180,"elapsed":9674136,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"40c18778-ed52-48b7-a0ff-d4ab49abfad6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|===================| 2973500/2984154 [09:03<00:01]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=10: 552.3\n"]},{"output_type":"stream","name":"stderr","text":["100%|===================| 2980726/2984154 [17:46<00:01]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=20: 1074.5\n"]},{"output_type":"stream","name":"stderr","text":["100%|===================| 2983663/2984154 [43:31<00:00]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=50: 2619.47\n"]},{"output_type":"stream","name":"stderr","text":["100%|===================| 2984031/2984154 [90:15<00:00]       "]},{"output_type":"stream","name":"stdout","text":["Background SHAP m=100: 5427.69\n"]}],"source":["# Note: this cell will take around 3 hours\n","background_shap_timings = {}\n","for head_size in [10,20,50,100]:\n","    start_time = time.time()\n","    explainer = shap.TreeExplainer(detection_model, detection_trainset.head(head_size), feature_perturbation='interventional')\n","    simple_shap_values = explainer.shap_values(detection_trainset)\n","    running_time = time.time() - start_time\n","    background_shap_timings[head_size] = running_time\n","    print(f\"Background SHAP m={head_size}: {round(running_time, 2)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bytCcou2rA6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752935655784,"user_tz":-180,"elapsed":3071455,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"793404d7-6972-4970-d2eb-21f64366f975"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path Dependent SHAP: 3071.4090366363525\n"]}],"source":["# Note: this cell will take around an hour\n","start_time = time.time()\n","explainer = shap.TreeExplainer(detection_model)\n","simple_shap_values = explainer.shap_values(detection_testset)\n","print(\"Path Dependent SHAP: \" + str(time.time() - start_time))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsaAvtPb2yTM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752935699951,"user_tz":-180,"elapsed":44171,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"d8712c0b-b48d-4924-986d-93b4d74f0d48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path Dependent SHAP IV n=10: 2.79\n","Path Dependent SHAP IV n=20: 4.97\n","Path Dependent SHAP IV n=50: 12.25\n","Path Dependent SHAP IV n=100: 24.16\n"]}],"source":["pd_shap_iv_timings = {}\n","for head_size in [10,20,50,100]:\n","    start_time = time.time()\n","    explainer = shap.TreeExplainer(detection_model)\n","    simple_shap_values = explainer.shap_interaction_values(detection_testset.head(head_size))\n","    running_time = time.time() - start_time\n","    pd_shap_iv_timings[head_size] = running_time\n","    print(f\"Path Dependent SHAP IV n={head_size}: {round(running_time, 2)}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RggLdGWn29sK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752984320101,"user_tz":-180,"elapsed":17,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"86228118-a9b3-4452-85c4-9b0aa9f77c72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Background SHAP estimated time: 265871649.54389995\n","Path-Dependent SHAP IV estimated time: 720971.6064\n"]}],"source":["print(f\"Background SHAP estimated time: {background_shap_timings[100] * (len(detection_trainset) / 100)}\")\n","print(f\"Path-Dependent SHAP IV estimated time: {pd_shap_iv_timings[100] * (len(detection_testset) / 100)}\")"]}],"metadata":{"colab":{"machine_shape":"hm","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyN/kCy/hbtAIPbW7cquyC9H"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}