{"cells":[{"cell_type":"markdown","metadata":{"id":"jJTxPcri_zok"},"source":["#Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9289,"status":"ok","timestamp":1752996525689,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"rmwF4lp3_sPo","outputId":"d284f735-3e9c-4bfa-fe3b-5945b3f80876"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n","Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n","Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n","Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n"]}],"source":["!pip install shap\n","!pip install xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-A1K_wP_3f4"},"outputs":[],"source":["import xgboost as xgb\n","import shap\n","import pandas as pd\n","import numpy as np\n","from typing import Union, Dict, Optional, Tuple, Set, List\n","from math import factorial\n","import time\n","from copy import copy\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.metrics import accuracy_score, f1_score\n","import scipy\n","\n","# For GPU execution\n","import cupy as cp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17524,"status":"ok","timestamp":1752996547396,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"},"user_tz":-180},"id":"3syONkVz_3dU","outputId":"bc0f0412-bfbb-48f9-c7c4-540ddc0b754e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Useful if you run this on google colab and downloaded the data into your drive.\n","# If you run the notebook in other environment remove these lines and change the 'pd.read_csv()' function in this notebook to read from\n","# where you saved you data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"HToRzUbmATa6"},"source":["## Environment Note\n","Can be run on the free 12GN RAM CPU runtime type.\n","No running time messurment are preformed in this notebook so the enviroment shouldn't effect its results."]},{"cell_type":"markdown","metadata":{"id":"xeNs6a-AApPC"},"source":["# WOODELF Code\n","\n","Same code as AAAI_WOODELF_CPU.ipynb"]},{"cell_type":"markdown","metadata":{"id":"8_OfQXy_ApGl"},"source":["### Decision Tree Ensemble Representation and Loading\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcpAxQwlASyo"},"outputs":[],"source":["from shap.explainers._tree import XGBTreeModelLoader # Use the shap package's XGBoost loading. this is cheating, I know...\n","\n","class DecisionTreeNode:\n","    \"\"\"\n","    Represent a decision tree node. Recursively the root node builds the tree structure (the root node knows it children and so on).\n","    Include several useful tree functions like BFS and n.split(c).\n","    \"\"\"\n","\n","    def __init__(\n","            self, feature_name: str, value: float, right: Optional[\"DecisionTreeNode\"], left: Optional[\"DecisionTreeNode\"], nan_go_left=True, index: int=None, cover=None,\n","            feature_contribution_replacement_values=None,\n","        ):\n","        \"\"\"\n","        See decision tree definition in the paper (Def. 6)\n","        The tree split function is a bit different as XGBoost also support NaN values.\n","        The split function is \"go left if df[feature_name]<value or (nan_go_left and df[feature_name] == NaN)\".\n","        Right and left parameters can be a DesicionTreeNode if this node is an inner node or None if the node is a leaf. (The leaf weight will be saved as the 'value')\n","        Cover is an optional parameter, it includes how many rows in the train set reached this node.\n","        \"\"\"\n","        self.index=index\n","        self.feature_name = feature_name\n","        self.value = float(value)\n","        self.right = right\n","        self.left = left\n","        self.nan_go_left = nan_go_left\n","        self.cover = cover\n","        self.consumer_pattern_to_characteristic_wdnf = None\n","        self.pc_pb_to_cube = None\n","        self.feature_contribution_replacement_values = feature_contribution_replacement_values\n","        self.parent = -1\n","        self.depth=None\n","\n","    def shall_go_left(self, row):\n","        \"\"\"\n","        This is the n.split(c) defined in Def.6\n","        \"\"\"\n","        if self.nan_go_left:\n","            return (row[self.feature_name] < self.value) | row[self.feature_name].isna()\n","        else:\n","            return row[self.feature_name] < self.value\n","\n","    def shall_go_right(self, row):\n","        return ~self.shall_go_left(row)\n","\n","    def GPU_shall_go_left(self, row):\n","        \"\"\"\n","        This is the n.split(c) defined in Def.6\n","        \"\"\"\n","        if self.nan_go_left:\n","            return (row[self.feature_name] < self.value) | cp.isnan(row[self.feature_name])\n","        else:\n","            return row[self.feature_name] < self.value\n","\n","    def GPU_shall_go_right(self, row):\n","        return ~self.GPU_shall_go_left(row)\n","\n","    def is_leaf(self):\n","        return self.right is None and self.left is None\n","\n","    def is_almost_leaf(self):\n","        return not self.is_leaf() and (self.right.is_leaf() or self.left.is_leaf())\n","\n","    def predict(self, data):\n","        if self.is_leaf():\n","            return pd.Series(self.value, index=data.index)\n","        return self.shall_go_left(data) * self.left.predict(data) + self.shall_go_right(data) * self.right.predict(data)\n","\n","    def bfs(self, including_myself: bool = True, including_leaves: bool = True):\n","        \"\"\"\n","        Return all the node children (and the node itself) in BFS order. The indexes should be in an increasing order.\n","        \"\"\"\n","        if self.is_leaf():\n","            return [self] if including_myself and including_leaves else []\n","\n","        children = [self] if including_myself else []\n","        nodes_to_visit = []\n","        if self.right is not None:\n","            nodes_to_visit.append(self.right)\n","        if self.left is not None:\n","            nodes_to_visit.append(self.left)\n","\n","        while len(nodes_to_visit) > 0:\n","            current_node = nodes_to_visit.pop(0)\n","            if current_node.right is not None:\n","                nodes_to_visit.append(current_node.right)\n","            if current_node.left is not None:\n","                nodes_to_visit.append(current_node.left)\n","\n","            if current_node.is_leaf():\n","                if including_leaves:\n","                    children.append(current_node)\n","            else:\n","                children.append(current_node)\n","\n","        return children\n","\n","    def get_all_leaves(self):\n","        children = self.bfs(including_leaves=True)\n","        return [node for node in children if node.is_leaf()]\n","\n","    def get_all_almost_leaves(self):\n","        children = self.bfs(including_leaves=True)\n","        return [node for node in children if node.is_almost_leaf()]\n","\n","    def get_all_features(self):\n","        inner_nodes = self.bfs(including_leaves=False)\n","        return set(n.feature_name for n in inner_nodes)\n","\n","    def get_all_leaves_with_path_to_root(self):\n","        nodes_to_visit = [(self, [])]\n","        leaves = []\n","        while len(nodes_to_visit) > 0:\n","            current_node, current_path_to_root = nodes_to_visit.pop(0)\n","            for next_node in [current_node.right, current_node.left]:\n","                next_node_obj = (next_node, current_path_to_root + [current_node.feature_name])\n","                if next_node.is_leaf():\n","                    leaves.append(next_node_obj)\n","                else:\n","                    nodes_to_visit.append(next_node_obj)\n","        return leaves\n","\n","    def __repr__(self):\n","        if self.is_leaf():\n","            return f\"{self.index} (cover: {self.cover}): leaf with value {self.value}\"\n","        return f\"{self.index} (cover: {self.cover}): {self.feature_name} < {self.value}\"\n","\n","def load_xgboost_tree(tree, features):\n","    \"\"\"\n","    Given an XGBoost Regressor tree, parse it and build a DecisionTreeNode object with it structure.\n","    Use the Tree object returned by the shap package's XGBTreeModelLoader class (given as the 'tree' parameter).\n","    The function also gets the training features.\n","    \"\"\"\n","    nodes = {}\n","    for index in range(len(tree.thresholds)):\n","        threshold = tree.thresholds[index]\n","        leaf_value = tree.values[index][0]\n","        if threshold == 0 and leaf_value != 0:\n","            value = leaf_value\n","        else:\n","            value = threshold\n","        nan_go_left = (tree.children_left[index] == tree.children_default[index])\n","        cover = tree.node_sample_weight[index]\n","        feature_index = tree.features[index]\n","        nodes[index] = DecisionTreeNode(\n","            feature_name=features[feature_index], value=value, right=None, left=None,\n","            nan_go_left=nan_go_left, index=index, cover=cover\n","        )\n","\n","    for index in range(len(tree.thresholds)):\n","        child_left = tree.children_left[index]\n","        child_right = tree.children_right[index]\n","\n","        if child_left != -1:\n","            nodes[index].left = nodes[child_left]\n","            nodes[child_left].parent = nodes[index]\n","        if child_right != -1:\n","            nodes[index].right = nodes[child_right]\n","            nodes[child_right].parent = nodes[index]\n","\n","    nodes[0].depth = tree.max_depth\n","    return nodes[0]\n","\n","\n","def load_xgboost_model(model, features):\n","    \"\"\"\n","    Load an XGBoost regressor tree (utilizing the shap python package parsing object)\n","    \"\"\"\n","    loader = XGBTreeModelLoader(model)\n","    return [load_xgboost_tree(t, features) for t in loader.get_trees()]"]},{"cell_type":"markdown","metadata":{"id":"2uLDFJehAzqi"},"source":["### Shapley/Banzhaf values and interaction values computation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8op_Nu3Axcx"},"outputs":[],"source":["def nCk(n, k):\n","    return factorial(n) // (factorial(k) * factorial(n-k))\n","\n","class CubeCharacteristicFunctionMetric(object):\n","    \"\"\"\n","    An abstruct class that calculate a metric on a cube/clause characteristic function.\n","    You can implement this class (override the calc_metric function) and then use this class and the WOODELF algorithm\n","    to calculate your metric effiecently on large background datasets.\n","\n","    Here, the metrics that inherit this class are: Shapley values, Shapley interaction values, Banzhaf values and Banzhaf interaction values\n","    \"\"\"\n","    INTERACTION_VALUE = False\n","    INTERACTION_VALUE_ONE_SIDE = False\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        raise NotImplemented()\n","\n","class ShapleyValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the linear-time formula for Shapley value computation on WDNF/WCNF, see Formula 3 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = False\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","\n","        s = s_plus | s_minus\n","        shapley_values = {}\n","\n","        # The new simple shapley values formula\n","        if len(s_plus) > 0:\n","            contribution = (1 / (len(s_plus) * nCk(len(s), len(s_plus))))\n","            for must_exist_feature in s_plus:\n","                shapley_values[must_exist_feature] = contribution\n","\n","        if len(s_minus) > 0:\n","            contribution = -1 / (len(s_minus) * nCk(len(s), len(s_minus)))\n","            for must_be_missing_feature in s_minus:\n","                shapley_values[must_be_missing_feature] = contribution\n","\n","        return shapley_values\n","\n","class ShapleyInteractionValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the formulas for Shapley interaction values computation on WDNF/WCNF, see Table 1 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = True\n","    INTERACTION_VALUE_ONE_SIDE = True\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","\n","        shapley_values = {}\n","        s = s_plus | s_minus\n","        if len(s_plus) > 0:\n","            # i,j in S+\n","            if len(s_plus) > 1:\n","                # 0.5 because the shapley interaction values in the shap package are actually divided by 2....\n","                contribution = 0.5 / ((len(s_plus) - 1) * nCk(len(s) - 1, len(s_plus) - 1))\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_plus:\n","                        if must_exists_feature < other_feature:\n","                            shapley_values[(must_exists_feature, other_feature)] = contribution\n","\n","            # i in S+   j in S-\n","            if len(s_minus) > 0:\n","                contribution = -0.5 / (len(s_minus) * nCk(len(s) - 1, len(s_minus)))\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_minus:\n","                        if must_exists_feature < other_feature:\n","                            shapley_values[(must_exists_feature, other_feature)] = contribution\n","\n","        if len(s_minus) > 0:\n","            # i,j in S-\n","            if len(s_minus) > 1:\n","                contribution = 0.5 / ((len(s_minus) - 1) * nCk(len(s) - 1, len(s_minus) - 1))\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_minus:\n","                        if must_be_missing_feature < other_feature:\n","                            shapley_values[(must_be_missing_feature, other_feature)] = contribution\n","            # i in S-   j in S+\n","            if len(s_plus) > 0:\n","                contribution = -0.5 / (len(s_plus) * nCk(len(s) - 1, len(s_plus)))\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_plus:\n","                        if must_be_missing_feature < other_feature:\n","                            shapley_values[(must_be_missing_feature, other_feature)] = contribution\n","        return shapley_values\n","\n","\n","class BanzahfValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the linear-time formula for Banzhaf value computation on WDNF/WCNF, see Formula 6 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = False\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","\n","        s = s_plus | s_minus\n","        banzhaf_values = {}\n","\n","        s_plus_contribution = 1 / (2 ** (len(s) - 1))\n","        s_minus_contribution = -s_plus_contribution\n","        # The new simple shapley values formula\n","        if len(s_plus) > 0:\n","            for must_exist_feature in s_plus:\n","                banzhaf_values[must_exist_feature] = s_plus_contribution\n","\n","        if len(s_minus) > 0:\n","            for must_be_missing_feature in s_minus:\n","                banzhaf_values[must_be_missing_feature] = s_minus_contribution\n","\n","        return banzhaf_values\n","\n","\n","class BanzhafInteractionValues(CubeCharacteristicFunctionMetric):\n","    \"\"\"\n","    Implement the formulas for Banzhaf interaction values computation on WDNF/WCNF, see Formula 7 in the paper.\n","    \"\"\"\n","    INTERACTION_VALUE = True\n","    INTERACTION_VALUE_ONE_SIDE = True\n","\n","    def calc_metric(self, s_plus, s_minus):\n","        if len(s_plus & s_minus) > 0:\n","            return {} # se and sne must be disjoint sets\n","        banzhaf_values = {}\n","\n","        contribution = (1 / (2 ** (len(s_plus) + len(s_minus) - 2)))\n","\n","        s = s_plus | s_minus\n","        if len(s_plus) > 0:\n","            # i,j in S+\n","            if len(s_plus) > 1:\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_plus:\n","                        if must_exists_feature < other_feature:\n","                            banzhaf_values[(must_exists_feature, other_feature)] = contribution\n","\n","            # i in S+   j in S-\n","            if len(s_minus) > 0:\n","                for must_exists_feature in s_plus:\n","                    for other_feature in s_minus:\n","                        if must_exists_feature < other_feature:\n","                            banzhaf_values[(must_exists_feature, other_feature)] = -contribution\n","\n","        if len(s_minus) > 0:\n","            # i,j in S-\n","            if len(s_minus) > 1:\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_minus:\n","                        if must_be_missing_feature < other_feature:\n","                            banzhaf_values[(must_be_missing_feature, other_feature)] = contribution\n","            # i in S-   j in S+\n","            if len(s_plus) > 0:\n","                for must_be_missing_feature in s_minus:\n","                    for other_feature in s_plus:\n","                        if must_be_missing_feature < other_feature:\n","                            banzhaf_values[(must_be_missing_feature, other_feature)] = -contribution\n","        return banzhaf_values"]},{"cell_type":"markdown","metadata":{"id":"LkktslZPA5R1"},"source":["### WOODELF CODE!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2eegCGdBv0x"},"outputs":[],"source":["class PathToValuesMatrix():\n","    \"\"\"\n","    An object that in charge of creating the M matrix for every leaf and feature.\n","    It takes the features along the root-to-leaf path and build the matrix (lines 7-16 in WOODELF pseudo code)\n","    The class also utilize the fact that the matrix only depends on the repitting sequence of the features along the path.\n","    For example the feature repetition sequence of [\"weight\", \"pluse\", \"age\", \"sex\", \"pluse\", \"sex\"] is [1, 2, 3, 4, 2, 4].\n","    All feature lists with this feature repetition sequence have the same set of matrixes.\n","\n","    This cache mechanism is improvement 2 in Sec. 9.1\n","    \"\"\"\n","    def __init__(self, metric: CubeCharacteristicFunctionMetric):\n","        self.metric = metric\n","\n","        self.cached_used = 0\n","        self.cache_miss = 0\n","        self.cache = {}\n","\n","\n","    def get_values_matrixes(self, features_in_path: List[str]):\n","        \"\"\"\n","        Apply the CubeCharacteristicFunctionMetric object (the v function), to create the matrixes M.\n","        Use the cache when possible, and update the cache with the created matrixes\n","        \"\"\"\n","        frs = self.get_feature_repetition_sequence(features_in_path)\n","\n","        frs_tuple = tuple(frs)\n","        if frs_tuple in self.cache:\n","            self.cached_used += 1\n","            matrixes = self.cache[frs_tuple]\n","        else:\n","            self.cache_miss += 1\n","            pc_pb_to_cube = self.map_patterns_to_cube(frs)\n","            matrixes = self.build_patterns_to_values_matrix(pc_pb_to_cube, self.metric, len(features_in_path))\n","            self.cache[frs_tuple] = matrixes\n","\n","        if not self.metric.INTERACTION_VALUE:\n","            matrixes_for_the_given_features = {features_in_path[index]: matrixes[index] for index in matrixes}\n","        else:\n","            matrixes_for_the_given_features = {}\n","            for feature_index_1, feature_index_2 in matrixes:\n","                the_right_index = (features_in_path[feature_index_1], features_in_path[feature_index_2])\n","\n","                # The feature_appearance can change the order of a feature pair (f1,f2) in one sided interaction metric, here we fix this.\n","                if self.metric.INTERACTION_VALUE_ONE_SIDE and features_in_path[feature_index_1] > features_in_path[feature_index_2]:\n","                    the_right_index = (features_in_path[feature_index_2], features_in_path[feature_index_1])\n","\n","                matrixes_for_the_given_features[the_right_index] = matrixes[(feature_index_1, feature_index_2)]\n","\n","        return matrixes_for_the_given_features\n","\n","    @staticmethod\n","    def get_feature_repetition_sequence(features_in_path: List[str]):\n","        \"\"\"\n","        Generate the feature repetition sequence.\n","        The math is simple, the feature at index i is replaced by i\n","        unless it appeared before in the sequance, in that case it will be represented by the index it already received.\n","\n","        Examples:\n","        [\"sex\", \"pluse\", \"age\", \"weight\", \"heart_rate\", \"sugar_in_blood\"] => [1, 2, 3, 4, 5, 6]\n","        [\"weight\", \"pluse\", \"age\", \"sex\", \"pluse\", \"sex\"] => [1, 2, 3, 4, 2, 4]\n","        \"\"\"\n","        feature_to_index = {}\n","        frs = []\n","        for i, feature in enumerate(features_in_path):\n","            if feature in feature_to_index:\n","                frs.append(feature_to_index[feature])\n","            else:\n","                feature_to_index[feature] = i\n","                frs.append(i)\n","\n","        return frs\n","\n","\n","    @staticmethod\n","    def build_patterns_to_values_matrix(dl, metric: CubeCharacteristicFunctionMetric, path_length):\n","        \"\"\"\n","        Apply the CubeCharacteristicFunctionMetric object (the v function), to create the matrixes M.\n","        include lines 12-16 in WOODELF pseudo code.\n","        dl is the returned mapping from the map_patterns_to_cube function\n","        \"\"\"\n","        matrix_details = {}\n","        for pc in dl:\n","            for pb in dl[pc]:\n","                s_plus, s_minus = dl[pc][pb]\n","                values = metric.calc_metric(s_plus, s_minus)\n","                for feature in values:\n","                    # Implement the line \"M[l][feature][p_c][p_b] = value\" in an efficient way that utilize the sparsity of M.\n","                    if feature not in matrix_details:\n","                        matrix_details[feature] = {\"pcs\": [], \"pbs\": [], \"values\": []}\n","                    matrix_details[feature][\"pcs\"].append(pc)\n","                    matrix_details[feature][\"pbs\"].append(pb)\n","                    matrix_details[feature][\"values\"].append(values[feature])\n","\n","        matrixs = {}\n","        for feature in matrix_details:\n","            # Save M as a sparse matrix (Improvement 1 in Sec. 9.1)\n","            matrix_values = (matrix_details[feature][\"values\"], (matrix_details[feature][\"pcs\"], matrix_details[feature][\"pbs\"]))\n","            matrixs[feature] = scipy.sparse.coo_matrix(matrix_values, shape=(2**path_length, 2**path_length), dtype=np.float32).tocsc()\n","        return matrixs\n","\n","    @staticmethod\n","    def map_patterns_to_cube(features_in_path: List[str]):\n","        \"\"\"\n","        The function MapPatternsToCube from Sect. 5 of the article.\n","        :params tree: The decision tree\n","        :params current_wdnf_table: The format is: wdnf_table[consumer_decision_pattern][background_decision_pattern] = (cube_positive_literals, cube_negative_literals)\n","        \"\"\"\n","        updated_wdnf_table = {0: {0: (set(), set())}}\n","        current_wdnf_table = None\n","        for feature in features_in_path:\n","            current_wdnf_table = updated_wdnf_table\n","            updated_wdnf_table = {}\n","            for consumer_pattern in current_wdnf_table:\n","                updated_wdnf_table[consumer_pattern * 2 + 0] = {}\n","                updated_wdnf_table[consumer_pattern * 2 + 1] = {}\n","                for background_pattern in current_wdnf_table[consumer_pattern]:\n","                    # Get the current cube (the possitive and negated literals) of the consumer and background patterns\n","                    s_plus, s_minus = current_wdnf_table[consumer_pattern][background_pattern]\n","                    # Implement the 4 rules\n","                    updated_wdnf_table[consumer_pattern * 2 + 1][background_pattern * 2 + 0] = (s_plus | {feature}, s_minus) # Rule 1\n","                    updated_wdnf_table[consumer_pattern * 2 + 0][background_pattern * 2 + 1] = (s_plus, s_minus | {feature}) # Rule 2\n","                    updated_wdnf_table[consumer_pattern * 2 + 1][background_pattern * 2 + 1] = (s_plus, s_minus) # Rule 3\n","\n","        return updated_wdnf_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IW3Nim0A488"},"outputs":[],"source":["\n","\n","def get_int_dtype_from_depth(depth):\n","    \"\"\"\n","    The decision pattern, when encoded as a number, have a bit for each node of the root-to-leaf-path.\n","    Choose the dtype according to the tree depth (a.k.a the max pattern length).\n","\n","    This is improvement 5 of Sec. 9.1\n","    \"\"\"\n","    if depth <= 8:\n","        return np.uint8\n","    if depth <= 16:\n","        return np.uint16\n","    if depth <= 32:\n","       return np.uint32\n","    return np.uint64\n","\n","\n","def GPU_get_int_dtype_from_depth(depth):\n","    \"\"\"\n","    Like get_int_dtype_from_depth but return CuPy types.\n","    \"\"\"\n","    if depth <= 8:\n","        return cp.uint8\n","    if depth <= 16:\n","        return cp.uint16\n","    if depth <= 32:\n","       return cp.uint32\n","    return cp.uint64\n","\n","\n","def calc_decision_patterns(tree, data, depth, GPU=False):\n","    \"\"\"\n","    An effiecent implementation of the CalcDecisionPatterns from Sec. 4 of the paper.\n","    \"\"\"\n","    # Use a tight uint type for efficiency. This is improvement 5 of Sec. 9.1\n","    int_dtype = GPU_get_int_dtype_from_depth(depth) if GPU else get_int_dtype_from_depth(depth)\n","\n","    leaves_patterns_dict = {} # This is the P_leaves mentioned in the paper\n","    inner_nodes_patterns_dict = {} # This is P_all\n","    if GPU:\n","        data_length = len(data[list(data.keys())[0]])\n","        inner_nodes_patterns_dict[tree.index] = cp.zeros(data_length, dtype=int_dtype)\n","    else:\n","        inner_nodes_patterns_dict[tree.index] = pd.Series(0, index=data.index).to_numpy().astype(int_dtype)\n","\n","    for current_node in tree.bfs():\n","        if current_node.is_leaf():\n","            leaves_patterns_dict[current_node.index] = inner_nodes_patterns_dict[current_node.index]\n","            continue\n","\n","        if GPU:\n","            left_bool_condition = current_node.GPU_shall_go_left(data)\n","            left_condition = left_bool_condition # .to_numpy().astype(int_dtype)\n","            right_condition = ~left_bool_condition # (~left_bool_condition).to_numpy().astype(int_dtype)\n","        else:\n","            left_bool_condition = current_node.shall_go_left(data)\n","            left_condition = left_bool_condition.to_numpy().astype(int_dtype)\n","            right_condition = (~left_bool_condition).to_numpy().astype(int_dtype)\n","        my_pattern = inner_nodes_patterns_dict[current_node.index]\n","        shifted_my_pattern = (my_pattern << 1)\n","        inner_nodes_patterns_dict[current_node.left.index] = shifted_my_pattern + left_condition\n","        inner_nodes_patterns_dict[current_node.right.index] = shifted_my_pattern + right_condition\n","    return leaves_patterns_dict\n","\n","\n","def preprocess_tree_background(tree: DecisionTreeNode, background_data: pd.DataFrame, depth: int, path_to_matrixes_calculator: PathToValuesMatrix, GPU=False):\n","    \"\"\"\n","    Run all the preprocessing needed given a tree and a background_data.\n","    Include lines 2-21 of the pseudo-code.\n","    \"\"\"\n","    background_patterns_matrix = calc_decision_patterns(tree, background_data, depth, GPU)\n","\n","    # Build f, implements lines 3-4 of the pseudo-code\n","    Frq_b = {}\n","    visited_leaves_parents = {}\n","    data_length = len(background_data) if not GPU else len(background_data[list(background_data.keys())[0]])\n","    for leaf, features_in_path in tree.get_all_leaves_with_path_to_root():\n","        if leaf.parent.index not in visited_leaves_parents:\n","            # np.bincount is a faster way to implement value_counts that uses the fact all decision patterns are integers between 0 and 2**depth\n","            if GPU:\n","                Frq_b[leaf.index] = cp.bincount(background_patterns_matrix[leaf.index], minlength=2**len(features_in_path))\n","                Frq_b[leaf.index] = Frq_b[leaf.index] / data_length\n","                Frq_b[leaf.index] = cp.asnumpy(Frq_b[leaf.index])\n","            else:\n","                Frq_b[leaf.index] = np.bincount(background_patterns_matrix[leaf.index], minlength=2**len(features_in_path))\n","                Frq_b[leaf.index] = Frq_b[leaf.index] / data_length\n","            visited_leaves_parents[leaf.parent.index] = Frq_b[leaf.index]\n","        else:\n","            # neighboor leaves have similar patterns (only the last bit is different)\n","            # For efficiency we reuse the frequencies computed for the neighboor.\n","\n","            # Given leaves l_i, l_{i+1} s.t. there is an inner node n where n.left = l_i and n.right=l_{i+1}.\n","            # The decision pattern of any consumer c in leaf l_i is the same as in leaf l_{i+1} except for the last bit which is different.\n","            # For example if the pattern of c and l_i is 010011011101 then the pattern of c and l_{i+1} is 010011011100 (the 1 in the end is replaced with 0)\n","            # Let the frequencies of l_i be [f1,f2,f3,f4,....,f_{n-1}, f_n], we can these conclude that the frequencies of l_{i+1} are [f2,f1,f4,f3,....,f_n, f_{n-1}].\n","            # We can find them by swapping any pair of numbers in the array.\n","            # The code below utilize this fact for efficiency - this saved half of the bincount opperations.\n","            # This trick is part of improvement 3 in Sec. 9.1 (this is the improvement to line 4)\n","            neighboor_frq = visited_leaves_parents[leaf.parent.index]\n","            frqs = []\n","            for i in range(0, len(neighboor_frq), 2):\n","                frqs.append(neighboor_frq[i+1])\n","                frqs.append(neighboor_frq[i])\n","            Frq_b[leaf.index] = np.array(frqs, dtype=np.float32)\n","\n","    for leaf, features_in_path in tree.get_all_leaves_with_path_to_root():\n","        # Build M, implements lines 7-16 of the pseudo-code\n","        matrixes = path_to_matrixes_calculator.get_values_matrixes(features_in_path)\n","\n","        # Build s, implements lines 17-21 of the pseudo-code\n","        features_to_values = {}\n","        fl = Frq_b[leaf.index]\n","        if GPU and 2**len(features_in_path) < len(fl): # this trim is needed only on GPU\n","            fl = fl[:2**len(features_in_path)]\n","        for feature in matrixes:\n","            # The matrix multiplication part is implemented in CPU, the matrix is too small for the GPU overhead to be worth it.\n","            # The sparse matrix multiplication here instade of the naive dense matrix multiplication is improvement 1 in Sec. 9.1\n","            features_to_values[feature] = matrixes[feature].dot(fl) * leaf.value\n","        leaf.feature_contribution_replacement_values = features_to_values\n","    return tree\n","\n","\n","def get_cupy_data(trees: List[DecisionTreeNode], df: pd.DataFrame):\n","    \"\"\"\n","    Cast the dataframe to cupy dict mapping between columns of CuPy arrays.\n","    We only do this for feature partisipating in the trees.\n","    \"\"\"\n","    data = {}\n","    for tree in trees:\n","        for feature in tree.get_all_features():\n","            if feature not in data:\n","                data[feature] = cp.asarray(df[feature].to_numpy())\n","    return data\n","\n","\n","def calculation_given_preprocessed_tree(tree: DecisionTreeNode, data: pd.DataFrame, shapley_values = None, depth: int = 6, GPU=False):\n","    \"\"\"\n","    Use the preprocessing to efficiently calculate the desired metric (Shapley/Banzahf values or interaction values)\n","    Implements lines 22-27 of the pseudo-code\n","    \"\"\"\n","    # line 22 of the pseudo-code\n","    decision_patterns = calc_decision_patterns(tree, data, depth, GPU)\n","\n","    # lines 23-27 of the pseudo-code\n","    if shapley_values is None:\n","        shapley_values = {}\n","\n","    for almost_leaf in tree.get_all_almost_leaves():\n","        if not almost_leaf.right.is_leaf() or not almost_leaf.left.is_leaf():\n","            # If only the right or the left node is a leaf use s as is\n","            leaf = almost_leaf.right if almost_leaf.right.is_leaf() else almost_leaf.left\n","            current_edp_indexes = decision_patterns[leaf.index]\n","            replacements_arrays = leaf.feature_contribution_replacement_values\n","        else:\n","            # If both the right and left nodes are leaves use improvement 3 of Sec. 9.1 (improvement of line 26)\n","            # See also the comment in preprocess_tree_background.\n","            # Given leaves l_i, l_{i+1} s.t. there is an inner node n where n.left=l_i and n.right=l_{i+1}.\n","            # mark the s vector of feature f and leaf l_i as s_i = [a1,a2,a3,...,an]\n","            # mark the s vector of feature f and leaf l_{i+1} as s_{i+1} = [b1,b2,b3,...,bn]\n","            # A trivial numpy indexing for feature f and the two leaves is [a1,a2,a3,...,an][ patterns ] + [b1,b2,b3,...,bn][ patterns ]\n","            # Utilizing the property explained in comment in preprocess_tree_background, we can run the equivalent numpy indexing:\n","            # [a1+b2, a2+b1, a3+b4, a4+b3,...,a_{n-1}+bn, an+b_{n-1}][ patterns ]\n","            # This saves half of the numpy indexing opperations\n","            current_edp_indexes = decision_patterns[almost_leaf.left.index]\n","            replacements_arrays = almost_leaf.left.feature_contribution_replacement_values\n","            for feature, replacement_values in almost_leaf.right.feature_contribution_replacement_values.items():\n","                values = []\n","                for i in range(0, len(replacement_values), 2):\n","                    values.append(replacement_values[i+1])\n","                    values.append(replacement_values[i])\n","\n","                if feature not in replacements_arrays:\n","                    replacements_arrays[feature] = np.array(values, dtype=np.float32)\n","                else:\n","                    replacements_arrays[feature] = np.array(values, dtype=np.float32) + replacements_arrays[feature]\n","\n","        for feature, replacement_values in replacements_arrays.items():\n","            if GPU:\n","                replacements_array = cp.asarray(replacement_values)\n","            else:\n","                replacements_array = np.ascontiguousarray(replacement_values)\n","\n","            # This is where the numpy indexing occur (improvement 6 of Sec. 9.1):\n","            current_shap_contribution = replacements_array[current_edp_indexes]\n","\n","            if feature not in shapley_values:\n","                shapley_values[feature] = current_shap_contribution\n","            else:\n","                shapley_values[feature] += current_shap_contribution\n","\n","    return shapley_values\n","\n","def shapley_value_calculation_given_preprocessed_tree_ensemble(\n","        preprocess_trees: List[DecisionTreeNode], consumer_data: pd.DataFrame, global_importance: bool = False, iv_one_sized: bool = False, GPU=False):\n","    \"\"\"\n","    Run desired metric calculation on a decision tree ensemble.\n","\n","    @param global_importance: Interation values can quickly fill up all the machine RAM, as there are quadratic number of them.\n","    To be able to run the algorithm on large datasets, when global_importance=True, we save only their sum of mean absolute values across the trees.\n","    While it makes the result not useful it let us run WOODELF on large datasets and test its running time.\n","    \"\"\"\n","    shapley_values = {}\n","    for tree in tqdm(preprocess_trees, desc=\"Computing SHAP\"):\n","        if global_importance:\n","            current_shapley_values = {}\n","            calculation_given_preprocessed_tree(tree, consumer_data, shapley_values=current_shapley_values, GPU=GPU)\n","            for key in current_shapley_values:\n","                if key not in shapley_values:\n","                    shapley_values[key] = 0\n","                shapley_values[key] += np.abs(current_shapley_values[key]).sum() / len(current_shapley_values[key])\n","        else:\n","            calculation_given_preprocessed_tree(tree, consumer_data, shapley_values=shapley_values, GPU=GPU)\n","\n","    # Improvement 4 of Sec. 9.1\n","    if iv_one_sized:\n","        all_keys = list(shapley_values.keys())\n","        for f1, f2 in all_keys:\n","            assert (f2,f1) not in shapley_values\n","            shapley_values[(f2, f1)] = shapley_values[(f1, f2)]\n","\n","    return shapley_values\n","\n","def calculate_background_shap(model: xgb.Booster, consumer_data: pd.DataFrame, background_data: pd.DataFrame, metric: CubeCharacteristicFunctionMetric, global_importance: bool = False, GPU=False):\n","    \"\"\"\n","    The WOODELF algorithm!!!\n","\n","    Gets an XGBoost regressor, consumer data of size n, background data for size m and a desired metric to calculate.\n","    Compute the desired metric in O(n+m)\n","    \"\"\"\n","    model_objs = load_xgboost_model(model, list(consumer_data.columns))\n","    path_to_matrixes_calculator = PathToValuesMatrix(metric=metric)\n","    if GPU:\n","        consumer_data = get_cupy_data(model_objs, consumer_data)\n","        background_data = get_cupy_data(model_objs, background_data)\n","    preprocessed_trees = []\n","    for tree in tqdm(model_objs, desc=\"Preprocessing the trees\"):\n","        preprocessed_trees.append(preprocess_tree_background(tree, background_data, depth=tree.depth, path_to_matrixes_calculator=path_to_matrixes_calculator, GPU=GPU))\n","\n","    print(f\"cache misses: {path_to_matrixes_calculator.cache_miss}, cache used: {path_to_matrixes_calculator.cached_used}\")\n","    shapley = shapley_value_calculation_given_preprocessed_tree_ensemble(\n","        preprocessed_trees, consumer_data, global_importance, iv_one_sized = metric.INTERACTION_VALUE_ONE_SIDE, GPU=GPU\n","    )\n","    return shapley"]},{"cell_type":"markdown","metadata":{"id":"a2HIfDe5A41Z"},"source":["### Path Dependent WOODELF Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVk5ky9mA4a3"},"outputs":[],"source":["def path_dependend_frequencies(tree: DecisionTreeNode, depth):\n","    \"\"\"\n","    Estimate the frequencies of the training data using the tree cover property.\n","    Implement Formula 9 of the article for all the leaves in the provided tree.\n","    \"\"\"\n","    if tree.is_leaf():\n","        return {tree.index: []}\n","\n","    leaves_freq_dict = {}\n","    inner_nodes_freq_dict = {}\n","    inner_nodes_freq_dict[tree.index] = [1]\n","    for current_node in tree.bfs():\n","        current_node_freq = inner_nodes_freq_dict[current_node.index]\n","        if current_node.is_leaf():\n","            leaves_freq_dict[current_node.index] = np.array(\n","                inner_nodes_freq_dict[current_node.index], dtype=np.float32\n","            )\n","            continue\n","\n","        freqs_l = []\n","        for freq in current_node_freq:\n","            freqs_l.append((current_node.right.cover/current_node.cover) * freq)\n","            freqs_l.append((current_node.left.cover/current_node.cover) * freq)\n","        inner_nodes_freq_dict[current_node.left.index] = freqs_l\n","\n","        freqs_r = []\n","        for freq in current_node_freq:\n","            # Changed the order of the 2 lines here, now left is first.\n","            freqs_r.append((current_node.left.cover/current_node.cover) * freq)\n","            freqs_r.append((current_node.right.cover/current_node.cover) * freq)\n","        inner_nodes_freq_dict[current_node.right.index] = freqs_r\n","    return leaves_freq_dict\n","\n","def fast_preprocess_path_dependent_shap(tree: DecisionTreeNode, path_to_matrixes_calculator: PathToValuesMatrix, depth=6):\n","    \"\"\"\n","    Implement the preprocssing needed for Path-Dependent WOODELF\n","    \"\"\"\n","    freq = path_dependend_frequencies(tree, depth)\n","    for leaf, features_in_path in tree.get_all_leaves_with_path_to_root():\n","        matrixes = path_to_matrixes_calculator.get_values_matrixes(features_in_path)\n","        features_to_values = {}\n","        for feature in matrixes:\n","            features_to_values[feature] = matrixes[feature].dot(freq[leaf.index]) * leaf.value\n","        leaf.feature_contribution_replacement_values = features_to_values\n","    return tree\n","\n","\n","def calculate_path_dependent_shap(model, consumer_data, metric: CubeCharacteristicFunctionMetric, global_importance: bool = False, GPU=False):\n","    \"\"\"\n","    Path-Dependent WOODELF algorithm!!\n","\n","    Given a model, a consumer data and a desired metric compute the metric under the Path-Dependent assumptions.\n","    \"\"\"\n","    model_objs = load_xgboost_model(model, list(consumer_data.columns))\n","    path_to_matrixes_calculator = PathToValuesMatrix(metric=metric)\n","    if GPU:\n","        consumer_data = get_cupy_data(model_objs, consumer_data)\n","\n","    preprocessed_trees = []\n","    for tree in tqdm(model_objs, desc=\"Preprocessing the trees\"):\n","        preprocessed_trees.append(fast_preprocess_path_dependent_shap(tree, path_to_matrixes_calculator=path_to_matrixes_calculator))\n","\n","    print(f\"cache misses: {path_to_matrixes_calculator.cache_miss}, cache used: {path_to_matrixes_calculator.cached_used}\")\n","    return shapley_value_calculation_given_preprocessed_tree_ensemble(preprocessed_trees, consumer_data, global_importance, iv_one_sized=metric.INTERACTION_VALUE_ONE_SIDE, GPU=GPU)\n"]},{"cell_type":"markdown","source":["# WOODELF Banzahf Correctness Verification"],"metadata":{"id":"qX0CENOJ8wM0"}},{"cell_type":"code","source":["import numpy as np\n","import xgboost as xgb\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, root_mean_squared_error\n","\n","# 1. Simulate a binary classification dataset with 10 features\n","X, y = make_classification(n_samples=10000,    # total samples\n","                           n_features=10,      # total features\n","                           n_informative=7,    # informative features\n","                           n_redundant=2,      # redundant features\n","                           n_classes=2,        # binary classification\n","                           random_state=42)\n","\n","# 2. Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                    test_size=0.2,\n","                                                    random_state=42)\n","\n","X_train = pd.DataFrame(X_train)\n","y_train = pd.Series(y_train)\n","X_test = pd.DataFrame(X_test)\n","y_test = pd.Series(y_test)\n","\n","XGB_PARAMS = {\n","    \"objective\": \"reg:squarederror\",  # Regression task with mean squared error loss\n","    \"eval_metric\": \"rmse\",  # Evaluation metric is root mean squared error\n","    \"max_depth\": 6,  # Maximum depth of each tree\n","    \"learning_rate\": 0.1,  # Learning rate (step size shrinkage)\n","    \"subsample\": 1,  # Subsample ratio of the training instances\n","    \"colsample_bytree\": 0.8,  # Subsample ratio of columns when constructing each tree\n","    \"seed\": 123,\n","    \"nthread\": 8,\n","}\n","\n","def xgboost_model(X_train, y_train, params, num_rounds=100):\n","    train_dmatrix = xgb.DMatrix(X_train, label=y_train)\n","    return xgb.train(params, train_dmatrix, num_rounds)\n","\n","# 3. Train an XGBoost classifier\n","model = xgboost_model(X_train, y_train, XGB_PARAMS, num_rounds=10)\n","\n","# 4. Evaluate accuracy on test set\n","y_pred = model.predict(xgb.DMatrix(X_test))\n","accuracy = root_mean_squared_error(y_test, y_pred)\n","\n","print(f\"Test accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJweWDtAtdZ_","executionInfo":{"status":"ok","timestamp":1752997062658,"user_tz":-180,"elapsed":988,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"62dd9883-be2c-44ca-c8c8-2dde95987f60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy: 0.3390\n"]}]},{"cell_type":"code","source":["banzhaf_values = calculate_background_shap(\n","    model, X_test.head(100), X_train.head(3), metric=BanzahfValues(), global_importance=False, GPU=False\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-N0xkGnvSH3","executionInfo":{"status":"ok","timestamp":1752997103663,"user_tz":-180,"elapsed":271,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"619baba0-8119-42cf-dd2d-d9bbae082517"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Preprocessing the trees: 100%|██████████| 10/10 [00:00<00:00, 56.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cache misses: 70, cache used: 520\n"]},{"output_type":"stream","name":"stderr","text":["Computing SHAP: 100%|██████████| 10/10 [00:00<00:00, 166.55it/s]\n"]}]},{"cell_type":"code","source":["pd.DataFrame(banzhaf_values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"5nqLb96T2YYM","executionInfo":{"status":"ok","timestamp":1752998457362,"user_tz":-180,"elapsed":84,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"ac9925af-d423-446c-c53a-77995855dd52"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           1         6         7         0         2         8         3  \\\n","0  -0.107780 -0.044294 -0.022663  0.007614  0.001210  0.009893  0.006814   \n","1   0.026543  0.144061  0.016670 -0.019270 -0.001669  0.179533  0.003668   \n","2  -0.091047 -0.061191 -0.011279  0.007920 -0.001310  0.013462  0.028254   \n","3   0.157058  0.044456  0.023814  0.056867  0.009921  0.006637  0.030301   \n","4  -0.041361  0.088578 -0.068861 -0.005071  0.004076  0.053165  0.018465   \n","..       ...       ...       ...       ...       ...       ...       ...   \n","95 -0.092646 -0.081030  0.078549  0.000596  0.003740  0.024106  0.004507   \n","96  0.229440  0.052311 -0.023703 -0.010677  0.010780  0.044019  0.079534   \n","97  0.116180  0.016294 -0.027227 -0.007331  0.001932 -0.018555  0.043838   \n","98  0.277076  0.020277  0.064941  0.011396  0.004754  0.013494  0.042965   \n","99 -0.143492 -0.055991 -0.019381 -0.010053  0.000555  0.045838  0.014851   \n","\n","           9         4         5  \n","0  -0.000523  0.009030 -0.000117  \n","1   0.000382  0.027379  0.015960  \n","2  -0.001047  0.007923  0.000000  \n","3   0.010208 -0.008228  0.098719  \n","4  -0.001429 -0.005595  0.004416  \n","..       ...       ...       ...  \n","95  0.000000  0.013070  0.022497  \n","96 -0.000891  0.012118  0.046506  \n","97  0.000000 -0.002818  0.005054  \n","98  0.000000  0.040704  0.013489  \n","99 -0.000523  0.010719 -0.012549  \n","\n","[100 rows x 10 columns]"],"text/html":["\n","  <div id=\"df-647bd6e9-c193-4308-80cd-e710f8e1afae\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>0</th>\n","      <th>2</th>\n","      <th>8</th>\n","      <th>3</th>\n","      <th>9</th>\n","      <th>4</th>\n","      <th>5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.107780</td>\n","      <td>-0.044294</td>\n","      <td>-0.022663</td>\n","      <td>0.007614</td>\n","      <td>0.001210</td>\n","      <td>0.009893</td>\n","      <td>0.006814</td>\n","      <td>-0.000523</td>\n","      <td>0.009030</td>\n","      <td>-0.000117</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.026543</td>\n","      <td>0.144061</td>\n","      <td>0.016670</td>\n","      <td>-0.019270</td>\n","      <td>-0.001669</td>\n","      <td>0.179533</td>\n","      <td>0.003668</td>\n","      <td>0.000382</td>\n","      <td>0.027379</td>\n","      <td>0.015960</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.091047</td>\n","      <td>-0.061191</td>\n","      <td>-0.011279</td>\n","      <td>0.007920</td>\n","      <td>-0.001310</td>\n","      <td>0.013462</td>\n","      <td>0.028254</td>\n","      <td>-0.001047</td>\n","      <td>0.007923</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.157058</td>\n","      <td>0.044456</td>\n","      <td>0.023814</td>\n","      <td>0.056867</td>\n","      <td>0.009921</td>\n","      <td>0.006637</td>\n","      <td>0.030301</td>\n","      <td>0.010208</td>\n","      <td>-0.008228</td>\n","      <td>0.098719</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.041361</td>\n","      <td>0.088578</td>\n","      <td>-0.068861</td>\n","      <td>-0.005071</td>\n","      <td>0.004076</td>\n","      <td>0.053165</td>\n","      <td>0.018465</td>\n","      <td>-0.001429</td>\n","      <td>-0.005595</td>\n","      <td>0.004416</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>-0.092646</td>\n","      <td>-0.081030</td>\n","      <td>0.078549</td>\n","      <td>0.000596</td>\n","      <td>0.003740</td>\n","      <td>0.024106</td>\n","      <td>0.004507</td>\n","      <td>0.000000</td>\n","      <td>0.013070</td>\n","      <td>0.022497</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>0.229440</td>\n","      <td>0.052311</td>\n","      <td>-0.023703</td>\n","      <td>-0.010677</td>\n","      <td>0.010780</td>\n","      <td>0.044019</td>\n","      <td>0.079534</td>\n","      <td>-0.000891</td>\n","      <td>0.012118</td>\n","      <td>0.046506</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>0.116180</td>\n","      <td>0.016294</td>\n","      <td>-0.027227</td>\n","      <td>-0.007331</td>\n","      <td>0.001932</td>\n","      <td>-0.018555</td>\n","      <td>0.043838</td>\n","      <td>0.000000</td>\n","      <td>-0.002818</td>\n","      <td>0.005054</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>0.277076</td>\n","      <td>0.020277</td>\n","      <td>0.064941</td>\n","      <td>0.011396</td>\n","      <td>0.004754</td>\n","      <td>0.013494</td>\n","      <td>0.042965</td>\n","      <td>0.000000</td>\n","      <td>0.040704</td>\n","      <td>0.013489</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>-0.143492</td>\n","      <td>-0.055991</td>\n","      <td>-0.019381</td>\n","      <td>-0.010053</td>\n","      <td>0.000555</td>\n","      <td>0.045838</td>\n","      <td>0.014851</td>\n","      <td>-0.000523</td>\n","      <td>0.010719</td>\n","      <td>-0.012549</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 10 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-647bd6e9-c193-4308-80cd-e710f8e1afae')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-647bd6e9-c193-4308-80cd-e710f8e1afae button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-647bd6e9-c193-4308-80cd-e710f8e1afae');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f65be485-d2a2-487c-8b7a-32405c1fd3e7\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f65be485-d2a2-487c-8b7a-32405c1fd3e7')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f65be485-d2a2-487c-8b7a-32405c1fd3e7 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.06523126363754272,\n          0.02983088791370392,\n          -0.12234874069690704\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.04068613797426224,\n          -0.042409736663103104,\n          -0.05536356568336487\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.01285078376531601,\n          -0.017808645963668823,\n          -0.013246157206594944\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          -0.01281925942748785,\n          0.00013347319327294827,\n          0.01339833252131939\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 95,\n        \"samples\": [\n          -0.015506045892834663,\n          -0.0025724167935550213,\n          0.0034167510457336903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0390293743565299,\n        \"min\": -0.025956664327168255,\n        \"max\": 0.17953290206787642,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          0.006650671421084553,\n          -0.018677536281757057,\n          0.024447923264233395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025948934993532086,\n        \"min\": -0.026276983572946243,\n        \"max\": 0.114788364560809,\n        \"num_unique_values\": 95,\n        \"samples\": [\n          0.02095713027908156,\n          0.018783414115508396,\n          0.0648637859764373\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          0.009317462332546711,\n          -0.0029318686574697495,\n          -0.0025496305897831917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020361889133943024,\n        \"min\": -0.07572469021154878,\n        \"max\": 0.09744655684335157,\n        \"num_unique_values\": 98,\n        \"samples\": [\n          0.009704472322482616,\n          0.003477096887460599,\n          0.012118262204846058\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.036868277109894154,\n        \"min\": -0.03298126805263261,\n        \"max\": 0.12845271335875927,\n        \"num_unique_values\": 94,\n        \"samples\": [\n          0.0046278671361505985,\n          -0.0061609843105543405,\n          -0.011270595054763058\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["import itertools\n","\n","def direct_banzhaf_for_feature(model, consumer_data, background_data, feature):\n","    other_features = [f for f in consumer_data.columns if f != feature]\n","    banzhaf_summation = None\n","    for baseline_index in range(len(background_data)):\n","        baseline = background_data.iloc[baseline_index]\n","        count = 0\n","        summation = None\n","        for subset in tqdm(itertools.product([True, False], repeat=len(other_features))):\n","            current_data = consumer_data.copy()\n","            for partisipate, f in zip(subset, other_features):\n","                if not partisipate:\n","                    current_data[f] = baseline[f]\n","\n","            i_partisipate_data = xgb.DMatrix(current_data)\n","            current_data = current_data.copy()\n","            current_data[feature] = baseline[feature]\n","            i_missing_data = xgb.DMatrix(current_data)\n","            diff = model.predict(i_partisipate_data) - model.predict(i_missing_data)\n","            if summation is None:\n","                summation = diff\n","            else:\n","                summation += diff\n","            count+=1\n","\n","        if banzhaf_summation is None:\n","            banzhaf_summation = summation / count\n","        else:\n","            banzhaf_summation += summation / count\n","\n","    return banzhaf_summation / len(background_data)\n","\n","\n"],"metadata":{"id":"QE1b9Fo6xSVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["consumer_data=X_test.head(100)\n","background_data=X_train.head(3)"],"metadata":{"id":"Xf0bE_8yymoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["direct_banzhaf_for_feature(model, consumer_data, background_data, 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LoZHklWuymrS","executionInfo":{"status":"ok","timestamp":1752998902543,"user_tz":-180,"elapsed":10030,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"253f8743-973d-4cce-c5d4-25227181dc22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["512it [00:04, 127.17it/s]\n","512it [00:02, 170.77it/s]\n","512it [00:02, 172.65it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 0.00761369, -0.01926973,  0.00792024,  0.05686681, -0.00507121,\n","       -0.00727398,  0.0012126 ,  0.00099663, -0.01000002,  0.04122552,\n","        0.0030938 ,  0.01920562, -0.02645049,  0.00479743,  0.00369598,\n","        0.01311891, -0.00949868,  0.0949232 ,  0.00816414, -0.01828076,\n","        0.02325188,  0.00818886,  0.00159699, -0.00171607,  0.00258602,\n","        0.00294186,  0.00740392, -0.0313449 ,  0.01189707, -0.01732039,\n","        0.03405893,  0.00762   ,  0.0406093 , -0.00387922,  0.00458481,\n","       -0.0085896 , -0.00947059,  0.01316916,  0.02935294,  0.00544161,\n","        0.00735509,  0.02815573, -0.00344228,  0.0085531 ,  0.00469485,\n","       -0.0014365 ,  0.00164981,  0.02371589,  0.00796844, -0.00652074,\n","        0.0335005 , -0.02742156,  0.01616387,  0.00013347,  0.020021  ,\n","        0.0082031 , -0.03253087, -0.00257825,  0.00769126, -0.00296253,\n","       -0.03973228, -0.00104011,  0.00790628, -0.00581197, -0.00184867,\n","       -0.00253093,  0.00854542,  0.00476497,  0.00230663,  0.01497541,\n","        0.01339835,  0.00994671,  0.01584301, -0.0128779 ,  0.0255727 ,\n","        0.01565008,  0.01737992,  0.00240772,  0.01094085,  0.00519099,\n","        0.05193171,  0.00696187, -0.0017109 , -0.01281923,  0.06509458,\n","        0.00067254,  0.00765622,  0.0172696 ,  0.02294797,  0.058862  ,\n","        0.05051409,  0.00480121,  0.0186346 ,  0.02901208,  0.0181284 ,\n","        0.00059562, -0.01067723, -0.00733068,  0.01139595, -0.01005318],\n","      dtype=float32)"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["banzhaf_values[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4ZPYoHOymw0","executionInfo":{"status":"ok","timestamp":1752998216584,"user_tz":-180,"elapsed":32,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"9f1a6345-5293-4ba7-bb22-284d7950d19a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.00761369, -0.01926972,  0.00792023,  0.05686682, -0.00507122,\n","       -0.00727397,  0.0012126 ,  0.00099665, -0.01000003,  0.04122552,\n","        0.00309379,  0.01920562, -0.02645054,  0.00479744,  0.00369598,\n","        0.01311891, -0.00949869,  0.09492327,  0.00816414, -0.01828076,\n","        0.02325189,  0.00818883,  0.00159698, -0.00171607,  0.00258602,\n","        0.00294186,  0.00740391, -0.03134489,  0.01189707, -0.01732038,\n","        0.03405894,  0.00761999,  0.04060932, -0.00387923,  0.00458478,\n","       -0.0085896 , -0.00947059,  0.01316916,  0.02935292,  0.00544162,\n","        0.0073551 ,  0.02815575, -0.00344228,  0.0085531 ,  0.00469484,\n","       -0.0014365 ,  0.00164981,  0.02371586,  0.00796844, -0.00652074,\n","        0.03350053, -0.02742163,  0.01616389,  0.00013347,  0.02002102,\n","        0.00820311, -0.03253086, -0.00257825,  0.00769125, -0.00296254,\n","       -0.03973232, -0.00104014,  0.00790628, -0.005812  , -0.00184867,\n","       -0.00253094,  0.00854544,  0.00476498,  0.00230664,  0.0149754 ,\n","        0.01339833,  0.00994672,  0.01584302, -0.01287798,  0.02557267,\n","        0.01565008,  0.01737992,  0.00240771,  0.01094087,  0.005191  ,\n","        0.05193173,  0.00696187, -0.0017109 , -0.01281926,  0.06509458,\n","        0.00067254,  0.00765623,  0.0172696 ,  0.02294797,  0.05886198,\n","        0.05051409,  0.0048012 ,  0.01863461,  0.02901207,  0.01812843,\n","        0.00059562, -0.01067723, -0.00733069,  0.01139594, -0.01005318],\n","      dtype=float32)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["for f in consumer_data.columns:\n","    print(f)\n","    direct_computation_banzhaf = direct_banzhaf_for_feature(model, consumer_data, background_data, f)\n","    pd.testing.assert_series_equal(\n","        pd.Series(direct_computation_banzhaf), pd.Series(banzhaf_values[f]),\n","        atol=0.00001, check_dtype=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K388tgLR4hEp","executionInfo":{"status":"ok","timestamp":1752999396930,"user_tz":-180,"elapsed":100683,"user":{"displayName":"ron wettenstein","userId":"07152022375387808485"}},"outputId":"9c6d470e-d74e-45be-c9b2-2918cd5c0735"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:04, 104.31it/s]\n","512it [00:03, 166.49it/s]\n","512it [00:03, 168.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["1\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:03, 131.94it/s]\n","512it [00:03, 150.94it/s]\n","512it [00:03, 170.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["2\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:02, 172.48it/s]\n","512it [00:04, 126.99it/s]\n","512it [00:03, 161.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["3\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:03, 170.61it/s]\n","512it [00:03, 168.06it/s]\n","512it [00:04, 124.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["4\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:03, 167.14it/s]\n","512it [00:03, 168.90it/s]\n","512it [00:02, 171.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["5\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:04, 122.99it/s]\n","512it [00:03, 169.23it/s]\n","512it [00:03, 170.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["6\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:03, 169.41it/s]\n","512it [00:04, 121.60it/s]\n","512it [00:02, 171.47it/s]\n"]},{"output_type":"stream","name":"stdout","text":["7\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:02, 173.10it/s]\n","512it [00:02, 172.79it/s]\n","512it [00:04, 123.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["8\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:03, 170.42it/s]\n","512it [00:03, 169.72it/s]\n","512it [00:02, 171.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["9\n"]},{"output_type":"stream","name":"stderr","text":["512it [00:04, 122.31it/s]\n","512it [00:03, 166.00it/s]\n","512it [00:03, 168.21it/s]\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyNSdfo+oSZT5BwT/rX60ROn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}